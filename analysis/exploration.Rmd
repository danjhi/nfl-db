---
title: "NFL-DB Exploration"
subtitle: "Prototyping calculated fields for players and teams"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(DBI)
library(RPostgres)
library(dplyr)
library(tidyverse)
library(dotenv)
library(nflverse)

# Load .env from project root
load_dot_env(file = here::here(".env"))

# Connect to Supabase Postgres
con <- dbConnect(
  Postgres(),
  host     = "db.twfzcrodldvhpfaykasj.supabase.co",
  port     = 5432,
  dbname   = "postgres",
  user     = "postgres",
  password = Sys.getenv("SUPABASE_DB_PASSWORD"),
  sslmode  = "require"
)
```

# Database Overview

```{r tables}
# List all public tables with row counts
dbGetQuery(con, "
  SELECT relname as table_name, n_live_tup as row_count
  FROM pg_stat_user_tables
  WHERE schemaname = 'public'
  ORDER BY n_live_tup DESC
")
```

# Players

```{r players}
players <- dbGetQuery(con, "SELECT * FROM players")
glimpse(players)
```

```{r player-positions}
players %>%
  count(position, sort = TRUE) %>%
  mutate(pct = round(n / sum(n) * 100, 1))
```

# Draft Picks

```{r draft-picks}
picks <- dbGetQuery(con, "
  SELECT dp.*, p.first_name, p.last_name, p.position
  FROM draft_picks dp
  JOIN players p ON dp.player_id = p.player_id
")
glimpse(picks)
```

# ADP

```{r adp}
adp <- dbGetQuery(con, "SELECT * FROM adp")
glimpse(adp)
```

# Player Seasons

```{r player-seasons}
player_seasons <- dbGetQuery(con, "SELECT * FROM player_seasons")
glimpse(player_seasons)
```

---

# Sandbox: Calculated Fields

Use the sections below to prototype calculated fields. Write your exploratory
code here, and when you're happy with the logic, ask Claude Code to read this
file and translate it to Python / SQL migrations.

## Player-Level Ideas

```{r player-calcs}
# Example: career draft capital (years * round * pick)
# Example: ADP trend across years
# Example: positional scarcity metrics

# Your code here...
```

## Team-Level Ideas

I will eventually use this database in my weekly daily fantasy content creation. Much of my weekly 
daily fantasy analysis comes down to projecting team level production with a very intense focus on variance. 
In these large daily fantasy contests, payouts are very top heavy, which means the focus is on chasing outlier type scenarios, both in terms of players hitting for larger than expected shares of their team's fantasy production and in terms of teams outperforming expectations. 

One of the ways I have done this in the past is to calculate offensive production numbers each week in each specific offensive category -- rushing fantasy points, passing fantasy points, receiving fantasy points. Then focus in on how much those numbers vary week to week. I do the same for each defense in terms of what they allow each week to opposing offenses. How those the offense and defensive production metrics interact both in terms of median/mean and variance is very important to me. Sample size is also a big deal, since there is so much turnover from one season to the next. Indicators that are strong late in the season are less so early in the year. Thus, I am interested in how much of a signal there is to be taken from previous seasons. Did the performance of a defense in 2024 give us any clues about how they were likely to perform early in 2025?

I am also interested in factors like each team's implied team total, how much they are favored by, the overall game total, and how those things impacted mean, median, and variance in the categories of interest.

I am interested in these numbers at both a team level and a positional level (QB, RB, WR, TE)

Within the nflreadr package in r, there are some very useful functions. For now, I want to just note those here. The first is load_ff_opportunity. Here are the columns that returns (for the current season):

```{r team-calcs}

load_ff_opportunity() |> 
  colnames()
```

game_id is important because it can tie back to the load_schedule function that contains the pre-game lines and over unders (which we can use to calculate implied team totals). We can also use this to figure out who the defense was in each game, which we need to calculate all of these stats for points against

posteam is the NFL team on offense (in possession of the ball). For now, I am only mildely interested in the expected metrics, but it may be worth calculating them for each category in case I want to explore it more in the future. The main thing here is pass fantasy points per team, rushing fantasy points per team, etc. I would like to have these for standard scoring, full PPR scoring, and half PPR scoring. We may just want to isolate the receptions column and then figure out standard fantasy points for each and go from there. 

Here is another function we could use to get these same stats (minus the expected numbers). It may or not be the better option. It does have fantasy_points and fantasy_points_ppr (fantasy points half ppr can be calculated as the mean between the two). 

```{r}
load_player_stats() |>
colnames()
```

### Step 1: Pull the raw data

We use `load_player_stats()` as our primary source because it already has `fantasy_points` (standard) and `fantasy_points_ppr`, plus all the component stats we need. We pull a full decade (2015-2025) to give us enough history for year-over-year signal analysis. We also pull `load_schedules()` to get the pre-game lines (spread, over/under) which we'll use to calculate implied team totals.

```{r pull-raw-data, cache=TRUE}
seasons <- 2015:2025

# Weekly player-level offensive stats
# Rename 'team' to 'recent_team' for consistency across all our joins
player_stats <- load_player_stats(seasons = seasons, stat_type = "offense") |>
  rename(recent_team = team)

# Game schedules with betting lines
schedules <- load_schedules(seasons = seasons)
```

### Step 2: Calculate fantasy point components by category

The key insight for DFS analysis is breaking total fantasy points into **passing**, **rushing**, and **receiving** components. This lets us see which category is driving a team's production and how volatile each category is independently. We also calculate half-PPR as the midpoint between standard and full PPR.

Standard scoring assumptions: pass TD = 4pt, rush/rec TD = 6pt, pass yd = 0.04pt/yd, rush/rec yd = 0.1pt/yd, INT = -2pt, fumble lost = -2pt.

Note for claude: It's a small thing, but don't forget two point conversions which are two points each for passing, rushing, and receiving. 

```{r fp-components}
player_stats <- player_stats |>
  mutate(
    # Half PPR total (midpoint of standard and full PPR)
    fantasy_points_hppr = (fantasy_points + fantasy_points_ppr) / 2,

    # Standard scoring components by category
    # Includes 2pt conversions (2pts each) and fumbles lost (-2pts each)
    fp_passing = passing_yards * 0.04 + passing_tds * 4 - passing_interceptions * 2
                 + passing_2pt_conversions * 2 - sack_fumbles_lost * 2,
    fp_rushing = rushing_yards * 0.1 + rushing_tds * 6
                 + rushing_2pt_conversions * 2 - rushing_fumbles_lost * 2,
    fp_receiving = receiving_yards * 0.1 + receiving_tds * 6
                   + receiving_2pt_conversions * 2 - receiving_fumbles_lost * 2,

    # Receiving components for PPR/half-PPR (add reception bonus)
    fp_receiving_ppr = fp_receiving + receptions * 1.0,
    fp_receiving_hppr = fp_receiving + receptions * 0.5
  )
```

### Step 3: Aggregate to team-week level

This is the core table -- one row per team per week with total fantasy points broken out by category. This is what we'll use to calculate means, variance, and everything else. We filter to regular season only (weeks 1-18) since that's what matters for DFS.

```{r team-week-agg}
team_week <- player_stats |>
  filter(week <= 18) |>
  group_by(recent_team, season, week) |>
  summarise(
    # Standard scoring by category
    team_fp_passing = sum(fp_passing, na.rm = TRUE),
    team_fp_rushing = sum(fp_rushing, na.rm = TRUE),
    team_fp_receiving = sum(fp_receiving, na.rm = TRUE),
    team_fp_total = sum(fantasy_points, na.rm = TRUE),

    # Full PPR
    team_fp_receiving_ppr = sum(fp_receiving_ppr, na.rm = TRUE),
    team_fp_total_ppr = sum(fantasy_points_ppr, na.rm = TRUE),

    # Half PPR
    team_fp_receiving_hppr = sum(fp_receiving_hppr, na.rm = TRUE),
    team_fp_total_hppr = sum(fantasy_points_hppr, na.rm = TRUE),
    .groups = "drop"
  )

glimpse(team_week)
```

### Step 4: Add game context (lines, implied totals)

For DFS, game environment matters enormously. A team's implied team total (derived from the spread and over/under) is the market's best guess at how many points they'll score. We want to know: when a team is in a high-total game or is heavily favored, how does that affect the distribution of their fantasy production?

We use nflreadr's `clean_homeaway()` to properly pivot the schedule into one-row-per-team format. The `spread_line` column represents how many points the home team is favored by (positive = home favored). We use that with `location` to compute each team's implied total.

Implied team total formula: `implied = (total + spread_from_team_perspective) / 2`. For example, if KC is home and `spread_line = 3` (KC favored by 3) in a game with total 46: KC implied = (46 + 3) / 2 = 24.5, BAL implied = (46 - 3) / 2 = 21.5.

```{r game-context}
# Use clean_homeaway() to pivot home/away into one-row-per-team
# spread_line: positive = home team favored (e.g., 3 means home -3)
game_context <- schedules |>
  filter(game_type == "REG") |>
  select(game_id, season, week, home_team, away_team,
         spread_line, total_line, home_score, away_score) |>
  clean_homeaway() |>
  mutate(
    # Implied team total using spread_line (positive = home favored)
    # Home team gets the boost, away team gets the reduction
    implied_total = (total_line + if_else(location == "home", spread_line, -spread_line)) / 2
  ) |>
  rename(actual_score = team_score, opp_score = opponent_score)

# Sanity check: week 1 2024 KC (home favorite by 3) vs BAL (away underdog)
game_context |>
  filter(season == 2024, week == 1, team %in% c("KC", "BAL")) |>
  select(team, opponent, location, spread_line, total_line, implied_total,
         actual_score, opp_score)

# Join context onto our team-week fantasy production
team_week <- team_week |>
  left_join(
    game_context |> select(season, week, team, opponent, location,
                           spread_line, total_line, implied_total,
                           actual_score, opp_score),
    by = c("recent_team" = "team", "season", "week")
  )

# Quick sanity check: how many rows have lines?
team_week |>
  summarise(
    total_rows = n(),
    has_lines = sum(!is.na(implied_total)),
    pct_with_lines = round(has_lines / total_rows * 100, 1)
  )
```

### Step 5: Defensive points allowed

This mirrors the offensive table but from the defense's perspective. Each defense's "points allowed" in a category is simply what the opposing offense scored in that category. This is critical for DFS -- we need to know not just how good an offense is, but how much the opposing defense gives up (and how variable that is).

```{r defensive-allowed}
# Create a defensive version: what did each team ALLOW?
# The opponent's offensive production = this team's defensive allowance
team_week_def <- team_week |>
  select(season, week, recent_team, opponent,
         starts_with("team_fp_")) |>
  rename_with(
    ~ str_replace(.x, "team_fp_", "def_allowed_fp_"),
    starts_with("team_fp_")
  )

# Join so each row now has BOTH:
# - This team's offensive production (team_fp_*)
# - What the opposing defense allowed (def_allowed_fp_*)
team_week <- team_week |>
  left_join(
    team_week_def |> select(-opponent),
    by = c("opponent" = "recent_team", "season", "week")
  )
```

### Step 6: Season-level summary with means and variance

This is where it gets interesting for DFS. For each team-season we calculate the mean and standard deviation of each category. The coefficient of variation (CV = SD/mean) normalizes the variance so we can compare across teams with different production levels. High CV = boom/bust = potentially interesting for tournaments.

```{r season-summary}
team_season_summary <- team_week |>
  group_by(recent_team, season) |>
  summarise(
    games = n(),

    # Offensive means (standard scoring)
    mean_fp_passing = mean(team_fp_passing),
    mean_fp_rushing = mean(team_fp_rushing),
    mean_fp_receiving = mean(team_fp_receiving),
    mean_fp_total = mean(team_fp_total),

    # Offensive standard deviations
    sd_fp_passing = sd(team_fp_passing),
    sd_fp_rushing = sd(team_fp_rushing),
    sd_fp_receiving = sd(team_fp_receiving),
    sd_fp_total = sd(team_fp_total),

    # Coefficient of variation (high = more boom/bust)
    cv_fp_passing = sd_fp_passing / mean_fp_passing,
    cv_fp_rushing = sd_fp_rushing / mean_fp_rushing,
    cv_fp_receiving = sd_fp_receiving / mean_fp_receiving,
    cv_fp_total = sd_fp_total / mean_fp_total,

    # Defensive means (what this team allowed)
    mean_def_passing = mean(def_allowed_fp_passing, na.rm = TRUE),
    mean_def_rushing = mean(def_allowed_fp_rushing, na.rm = TRUE),
    mean_def_receiving = mean(def_allowed_fp_receiving, na.rm = TRUE),
    mean_def_total = mean(def_allowed_fp_total, na.rm = TRUE),

    # Defensive standard deviations
    sd_def_passing = sd(def_allowed_fp_passing, na.rm = TRUE),
    sd_def_rushing = sd(def_allowed_fp_rushing, na.rm = TRUE),
    sd_def_receiving = sd(def_allowed_fp_receiving, na.rm = TRUE),
    sd_def_total = sd(def_allowed_fp_total, na.rm = TRUE),

    .groups = "drop"
  )

# Top offenses in 2024 by total FP with variance
team_season_summary |>
  filter(season == 2024) |>
  arrange(desc(mean_fp_total)) |>
  select(recent_team, games, mean_fp_total, sd_fp_total, cv_fp_total,
         mean_fp_passing, mean_fp_rushing, mean_fp_receiving) |>
  head(10)
```

```{r worst-defenses-2024}
# Worst defenses in 2024 (most FP allowed = best matchups for DFS)
team_season_summary |>
  filter(season == 2024) |>
  arrange(desc(mean_def_total)) |>
  select(recent_team, mean_def_total, sd_def_total,
         mean_def_passing, mean_def_rushing, mean_def_receiving) |>
  head(10)
```

### Step 7: Positional breakdown (QB, RB, WR, TE per team-week)

For DFS stacking, we need to know not just total team production but how it distributes across positions. A team that funnels production through WRs has a different stacking profile than one that's RB-heavy. We also calculate each position's share of total team FP.

```{r positional-breakdown}
pos_team_week <- player_stats |>
  filter(week <= 18, position_group %in% c("QB", "RB", "WR", "TE")) |>
  group_by(recent_team, season, week, position_group) |>
  summarise(
    pos_fp = sum(fantasy_points, na.rm = TRUE),
    pos_fp_ppr = sum(fantasy_points_ppr, na.rm = TRUE),
    pos_fp_hppr = sum(fantasy_points_hppr, na.rm = TRUE),
    .groups = "drop"
  )

# Add total team production and calculate position share
pos_team_week <- pos_team_week |>
  left_join(
    team_week |> select(recent_team, season, week, team_fp_total),
    by = c("recent_team", "season", "week")
  ) |>
  mutate(pos_share = pos_fp / team_fp_total)

# Season-level positional summary
pos_season <- pos_team_week |>
  group_by(recent_team, season, position_group) |>
  summarise(
    mean_pos_fp = mean(pos_fp),
    sd_pos_fp = sd(pos_fp),
    cv_pos_fp = sd_pos_fp / mean_pos_fp,
    mean_pos_share = mean(pos_share),
    .groups = "drop"
  )

# 2024 positional shares by team (which teams funnel to which positions?)
pos_season |>
  filter(season == 2024) |>
  select(recent_team, position_group, mean_pos_fp, mean_pos_share) |>
  pivot_wider(
    names_from = position_group,
    values_from = c(mean_pos_fp, mean_pos_share),
    names_sep = "_"
  ) |>
  arrange(desc(mean_pos_fp_WR))
```

### Step 8: Game environment impact on production

How does implied total / spread affect production and variance? This is key for DFS -- when Vegas expects a shootout, does that actually translate to higher (and more variable) fantasy production?

```{r game-environment}
# Bucket games by implied total
team_week_bucketed <- team_week |>
  filter(!is.na(implied_total)) |>
  mutate(
    implied_bucket = cut(implied_total,
      breaks = c(0, 18, 21, 24, 27, 35),
      labels = c("< 18", "18-21", "21-24", "24-27", "27+")
    ),
    # Team spread: positive = favored (home gets spread_line, away gets negated)
    team_spread = if_else(location == "home", spread_line, -spread_line),
    spread_bucket = cut(team_spread,
      breaks = c(-Inf, -7, -3, 0, 3, 7, Inf),
      labels = c("Dog 7+", "Dog 3-7", "Dog 0-3",
                 "Fav 0-3", "Fav 3-7", "Fav 7+")
    )
  )

# How does implied total affect production?
team_week_bucketed |>
  group_by(implied_bucket) |>
  summarise(
    n_games = n(),
    mean_total = mean(team_fp_total),
    sd_total = sd(team_fp_total),
    mean_passing = mean(team_fp_passing),
    mean_rushing = mean(team_fp_rushing),
    mean_receiving = mean(team_fp_receiving),
    .groups = "drop"
  )
```

```{r spread-impact}
# How does spread affect production?
team_week_bucketed |>
  group_by(spread_bucket) |>
  summarise(
    n_games = n(),
    mean_total = mean(team_fp_total),
    sd_total = sd(team_fp_total),
    mean_passing = mean(team_fp_passing),
    mean_rushing = mean(team_fp_rushing),
    .groups = "drop"
  )
```

### Step 9: Year-over-year signal (does last season predict this season?)

This is the sample size question. If a defense was terrible in 2024, should we expect them to still be bad in weeks 1-4 of 2025? We test this by correlating full-season metrics with early-season performance the following year. High correlation = more predictive signal we can use early in the season.

```{r yoy-signal}
# Compare: full season N performance vs first 4 weeks of season N+1
early_season <- team_week |>
  filter(week <= 4) |>
  group_by(recent_team, season) |>
  summarise(
    early_off_total = mean(team_fp_total),
    early_def_total = mean(def_allowed_fp_total, na.rm = TRUE),
    .groups = "drop"
  )

yoy <- team_season_summary |>
  select(recent_team, season,
         mean_fp_total, mean_def_total) |>
  inner_join(
    early_season |> mutate(season = season - 1),
    by = c("recent_team", "season")
  )

# Offense: does prior year full season predict next year weeks 1-4?
cat("Offense YoY correlation (full season → next year wk 1-4):",
    round(cor(yoy$mean_fp_total, yoy$early_off_total, use = "complete.obs"), 3), "\n")

# Defense: does prior year full season predict next year weeks 1-4?
cat("Defense YoY correlation (full season → next year wk 1-4):",
    round(cor(yoy$mean_def_total, yoy$early_def_total, use = "complete.obs"), 3), "\n")
```

```{r yoy-weekly-decay}
# How does the signal decay? Check correlation at different week cutoffs.
# This tells us: at what point in the new season does current-year data
# become more useful than last year's data?
yoy_by_week <- map_dfr(1:17, function(wk) {
  early <- team_week |>
    filter(week <= wk) |>
    group_by(recent_team, season) |>
    summarise(
      early_off = mean(team_fp_total),
      early_def = mean(def_allowed_fp_total, na.rm = TRUE),
      .groups = "drop"
    ) |>
    mutate(season = season - 1)

  joined <- team_season_summary |>
    select(recent_team, season, mean_fp_total, mean_def_total) |>
    inner_join(early, by = c("recent_team", "season"))

  tibble(
    through_week = wk,
    off_cor = cor(joined$mean_fp_total, joined$early_off, use = "complete.obs"),
    def_cor = cor(joined$mean_def_total, joined$early_def, use = "complete.obs")
  )
})

# Plot: when does prior year signal fade?
yoy_by_week |>
  pivot_longer(cols = c(off_cor, def_cor), names_to = "side", values_to = "correlation") |>
  ggplot(aes(x = through_week, y = correlation, color = side)) +
  geom_line(linewidth = 1) +
  geom_point() +
  labs(
    title = "Prior Year Signal Decay",
    subtitle = "Correlation between full prior season and current season through week N",
    x = "Current season through week...",
    y = "Correlation with prior year full season"
  ) +
  scale_color_manual(values = c("off_cor" = "steelblue", "def_cor" = "firebrick"),
                     labels = c("off_cor" = "Offense", "def_cor" = "Defense")) +
  theme_minimal()
```


### Step 10: Predict 2026 team-level baselines

Using the year-over-year correlations from Step 9, we can project 2026 offensive and defensive fantasy point baselines for each team. The method is simple regression toward the mean: a team's projected 2026 value is a weighted blend of their 2025 performance and the league average, where the weight is the YoY correlation.

Formula: `projected = league_mean + r * (team_2025 - league_mean)` where `r` is the YoY correlation.

This means: if offense has r=0.40, a team that was 10 FP above average in 2025 is projected to be ~4 FP above average in 2026. Defense (r=0.26) regresses even harder — 10 FP above average becomes ~2.6 FP above average.

```{r predict-2026}
# Get YoY correlations (full season N → full season N+1)
yoy_full <- team_season_summary |>
  inner_join(
    team_season_summary |>
      mutate(season = season - 1) |>
      rename(next_off = mean_fp_total, next_def = mean_def_total),
    by = c("recent_team", "season")
  )

r_offense <- cor(yoy_full$mean_fp_total, yoy_full$next_off, use = "complete.obs")
r_defense <- cor(yoy_full$mean_def_total, yoy_full$next_def, use = "complete.obs")

cat("YoY correlations used for regression:\n")
cat("  Offense:", round(r_offense, 3), "\n")
cat("  Defense:", round(r_defense, 3), "\n\n")

# 2025 team averages + league means
team_2025 <- team_season_summary |>
  filter(season == 2025)

league_mean_off <- mean(team_2025$mean_fp_total)
league_mean_def <- mean(team_2025$mean_def_total)

cat("2025 league averages:\n")
cat("  Offense:", round(league_mean_off, 1), "FP/game\n")
cat("  Defense:", round(league_mean_def, 1), "FP allowed/game\n\n")

# Project 2026: regress toward league mean
projections_2026 <- team_2025 |>
  mutate(
    proj_off_2026 = league_mean_off + r_offense * (mean_fp_total - league_mean_off),
    proj_def_2026 = league_mean_def + r_defense * (mean_def_total - league_mean_def),
    # How far above/below average is each projection?
    off_edge = proj_off_2026 - league_mean_off,
    def_edge = proj_def_2026 - league_mean_def
  ) |>
  select(recent_team, mean_fp_total, proj_off_2026, off_edge,
         mean_def_total, proj_def_2026, def_edge) |>
  arrange(desc(proj_off_2026))

cat("=== 2026 Projected Offensive FP/game (sorted best to worst) ===\n")
projections_2026 |>
  select(recent_team, `2025 actual` = mean_fp_total,
         `2026 proj` = proj_off_2026, `edge` = off_edge) |>
  mutate(across(where(is.numeric), ~ round(.x, 1))) |>
  print(n = 32)

cat("\n=== 2026 Projected Defensive FP Allowed/game (sorted worst to best) ===\n")
projections_2026 |>
  arrange(desc(proj_def_2026)) |>
  select(recent_team, `2025 actual` = mean_def_total,
         `2026 proj` = proj_def_2026, `edge` = def_edge) |>
  mutate(across(where(is.numeric), ~ round(.x, 1))) |>
  print(n = 32)
```

```{r predict-2026-category}
# Same approach but by fantasy point category (passing, rushing, receiving)
# First get category-level season summaries and YoY correlations
team_season_cats <- team_week |>
  group_by(recent_team, season) |>
  summarise(
    mean_passing = mean(team_fp_passing),
    mean_rushing = mean(team_fp_rushing),
    mean_receiving = mean(team_fp_receiving),
    mean_def_passing = mean(def_allowed_fp_passing, na.rm = TRUE),
    mean_def_rushing = mean(def_allowed_fp_rushing, na.rm = TRUE),
    mean_def_receiving = mean(def_allowed_fp_receiving, na.rm = TRUE),
    .groups = "drop"
  )

# Category YoY correlations
yoy_cats <- team_season_cats |>
  inner_join(
    team_season_cats |> mutate(season = season - 1) |>
      rename_with(~ paste0("next_", .x), -c(recent_team, season)),
    by = c("recent_team", "season")
  )

cat("=== Category-Level YoY Correlations ===\n")
cat("Offense:\n")
cat("  Passing: ", round(cor(yoy_cats$mean_passing, yoy_cats$next_mean_passing, use="complete.obs"), 3), "\n")
cat("  Rushing: ", round(cor(yoy_cats$mean_rushing, yoy_cats$next_mean_rushing, use="complete.obs"), 3), "\n")
cat("  Receiving:", round(cor(yoy_cats$mean_receiving, yoy_cats$next_mean_receiving, use="complete.obs"), 3), "\n")
cat("Defense:\n")
cat("  Pass allowed: ", round(cor(yoy_cats$mean_def_passing, yoy_cats$next_mean_def_passing, use="complete.obs"), 3), "\n")
cat("  Rush allowed: ", round(cor(yoy_cats$mean_def_rushing, yoy_cats$next_mean_def_rushing, use="complete.obs"), 3), "\n")
cat("  Rec allowed:  ", round(cor(yoy_cats$mean_def_receiving, yoy_cats$next_mean_def_receiving, use="complete.obs"), 3), "\n")
```

## Deeper Exploration: Rolling Averages, Opponent Adjustment, and Prediction

The goal of this section is to build toward a model that predicts team-level fantasy points by category for an upcoming game. The key ingredients:

1. **Rolling averages** — How has this team performed over their last N games? (across seasons)
2. **Opponent adjustment** — How much of a team's output was "them" vs "the matchup"?
3. **Window size analysis** — How many prior games give us the best signal?
4. **Prediction model** — Combine offensive rolling avg + opponent defensive rolling avg to predict next game

### Step 11: Create chronological game log with rolling averages

First we need a single chronological game log per team that spans seasons. This lets us compute rolling averages that carry over from late 2024 into early 2025, etc. We compute rolling averages at multiple window sizes (3, 5, 8, 10, 16 games) for each category.

```{r rolling-setup}
library(slider)  # for slide_dbl (rolling window functions)

# Build a chronological game log per team
# team_week already has season, week, recent_team, and all the FP columns
game_log <- team_week |>
  filter(!is.na(opponent)) |>
  arrange(recent_team, season, week) |>
  group_by(recent_team) |>
  mutate(game_num = row_number()) |>
  ungroup()

cat("Game log:", nrow(game_log), "team-games across",
    n_distinct(game_log$season), "seasons\n")
cat("Games per team: min =", min(table(game_log$recent_team)),
    "max =", max(table(game_log$recent_team)), "\n")

# Validation: each team should have ~17-18 games per season
game_log |>
  count(recent_team, season) |>
  summarise(min_games = min(n), max_games = max(n), mean_games = round(mean(n), 1))
```

```{r rolling-averages}
# Compute PRIOR rolling averages (exclude current game — we're predicting it)
# slide_dbl with .before = N-1, .complete = TRUE gives us N-game lookback
windows <- c(3, 5, 8, 10, 16)

# Offensive rolling averages by category
for (w in windows) {
  game_log <- game_log |>
    group_by(recent_team) |>
    mutate(
      "roll_{w}_off_total" := slide_dbl(lag(team_fp_total), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_off_passing" := slide_dbl(lag(team_fp_passing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_off_rushing" := slide_dbl(lag(team_fp_rushing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_off_receiving" := slide_dbl(lag(team_fp_receiving), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_total" := slide_dbl(lag(def_allowed_fp_total), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_passing" := slide_dbl(lag(def_allowed_fp_passing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_rushing" := slide_dbl(lag(def_allowed_fp_rushing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_receiving" := slide_dbl(lag(def_allowed_fp_receiving), mean, .before = w - 1, .complete = TRUE)
    ) |>
    ungroup()
}

# Validation: spot check — KC's 5-game rolling offensive total going into week 6, 2024
kc_check <- game_log |>
  filter(recent_team == "KC", season == 2024, week %in% 1:6) |>
  select(season, week, team_fp_total, roll_5_off_total)
cat("\nValidation: KC 2024 weeks 1-6\n")
cat("Weeks 1-5 actual FP:", paste(round(kc_check$team_fp_total[1:5], 1), collapse = ", "), "\n")
cat("Mean of weeks 1-5:", round(mean(kc_check$team_fp_total[1:5]), 1), "\n")
cat("roll_5_off_total at week 6:", round(kc_check$roll_5_off_total[6], 1), "\n")
```

### Step 12: Opponent-adjusted metrics

Raw production is misleading — scoring 80 FP against the league's worst defense means less than scoring 65 against the best. We adjust by comparing each game to the opponent's rolling average:

`adjusted = actual - opponent_rolling_def_avg + league_avg`

This normalizes for schedule strength. A team that consistently beats expectation has positive adjustment; one inflated by weak opponents regresses.

```{r opponent-adjusted}
# We need the OPPONENT's rolling defensive average for each game
# Join opponent's rolling def stats onto each game
# Using 8-game window as our primary (will test others later)

# First, create opponent lookup: for each team-game, what was the opponent's
# rolling defensive average going INTO that game?
opp_def_lookup <- game_log |>
  select(recent_team, season, week,
         starts_with("roll_")) |>
  select(recent_team, season, week, matches("roll_.*_def_"))

# Join: for each row, get the OPPONENT's rolling defensive averages
game_log <- game_log |>
  left_join(
    opp_def_lookup |> rename_with(~ str_replace(.x, "roll_", "opp_roll_"), starts_with("roll_")),
    by = c("opponent" = "recent_team", "season", "week")
  )

# Calculate league average FP per game (for normalization baseline)
league_avg <- game_log |>
  summarise(
    lg_total = mean(team_fp_total),
    lg_passing = mean(team_fp_passing),
    lg_rushing = mean(team_fp_rushing),
    lg_receiving = mean(team_fp_receiving)
  )

cat("League averages (FP/game):\n")
cat("  Total:", round(league_avg$lg_total, 1), "\n")
cat("  Passing:", round(league_avg$lg_passing, 1), "\n")
cat("  Rushing:", round(league_avg$lg_rushing, 1), "\n")
cat("  Receiving:", round(league_avg$lg_receiving, 1), "\n")

# Opponent-adjusted offensive production (using 8-game opponent rolling avg)
# adjusted = actual - opp_def_rolling_avg + league_avg
# Interpretation: "what would this team have scored against an average defense?"
game_log <- game_log |>
  mutate(
    adj_off_total = team_fp_total - opp_roll_8_def_total + league_avg$lg_total,
    adj_off_passing = team_fp_passing - opp_roll_8_def_passing + league_avg$lg_passing,
    adj_off_rushing = team_fp_rushing - opp_roll_8_def_rushing + league_avg$lg_rushing,
    adj_off_receiving = team_fp_receiving - opp_roll_8_def_receiving + league_avg$lg_receiving
  )

# Validation: compare raw vs adjusted for a team with easy/hard schedule
cat("\n2024 season averages — raw vs opponent-adjusted (total FP):\n")
game_log |>
  filter(season == 2024, !is.na(adj_off_total)) |>
  group_by(recent_team) |>
  summarise(
    raw_mean = mean(team_fp_total),
    adj_mean = mean(adj_off_total),
    diff = adj_mean - raw_mean,
    .groups = "drop"
  ) |>
  arrange(desc(abs(diff))) |>
  mutate(across(where(is.numeric), ~ round(.x, 1))) |>
  head(10) |>
  print()
```

### Step 13: Predictive power by window size

The key question: how well do rolling averages predict the NEXT game? We test each window size and compare raw vs opponent-adjusted. We also look at how much each additional week of data improves our prediction.

```{r predictive-power}
# For each window size, correlate rolling average with actual next-game production
# We already have rolling averages computed; actual production is in the same row
# (rolling avg is PRIOR games, actual is current game)

results <- map_dfr(windows, function(w) {
  col_off <- paste0("roll_", w, "_off_total")
  col_def <- paste0("roll_", w, "_def_total")

  valid <- game_log |> filter(!is.na(.data[[col_off]]) & !is.na(.data[[col_def]]))

  tibble(
    window = w,
    n_games = nrow(valid),
    # How well does team's own rolling avg predict their next game?
    r_own_off = cor(valid[[col_off]], valid$team_fp_total, use = "complete.obs"),
    # How well does opponent's rolling def avg predict what team scores?
    r_opp_def = cor(valid[[paste0("opp_roll_", w, "_def_total")]],
                    valid$team_fp_total, use = "complete.obs"),
    # Combined: own offense + opponent defense
    r_combined = cor(
      valid[[col_off]] - valid[[paste0("opp_roll_", w, "_def_total")]] + league_avg$lg_total,
      valid$team_fp_total,
      use = "complete.obs"
    )
  )
})

cat("=== Correlation with Next-Game Total FP by Window Size ===\n")
results |>
  mutate(across(starts_with("r_"), ~ round(.x, 3))) |>
  print()

# Validation: r_own_off should increase with window size (more data = more signal)
# r_combined should be higher than either individual predictor
cat("\nValidation checks:\n")
cat("  Own off r increases with window? ",
    all(diff(results$r_own_off) >= -0.01), "\n")  # allow tiny wobble
cat("  Combined > own off alone? ",
    all(results$r_combined > results$r_own_off), "\n")
```

```{r predictive-by-category}
# Same analysis but broken out by category
cat_results <- map_dfr(windows, function(w) {
  valid <- game_log |>
    filter(!is.na(.data[[paste0("roll_", w, "_off_total")]]))

  cats <- c("passing", "rushing", "receiving")
  map_dfr(cats, function(cat) {
    off_col <- paste0("roll_", w, "_off_", cat)
    opp_col <- paste0("opp_roll_", w, "_def_", cat)
    actual_col <- paste0("team_fp_", cat)
    lg_col <- paste0("lg_", cat)

    v <- valid |> filter(!is.na(.data[[off_col]]) & !is.na(.data[[opp_col]]))

    tibble(
      window = w,
      category = cat,
      n = nrow(v),
      r_own = cor(v[[off_col]], v[[actual_col]], use = "complete.obs"),
      r_opp = cor(v[[opp_col]], v[[actual_col]], use = "complete.obs"),
      r_combined = cor(
        v[[off_col]] - v[[opp_col]] + league_avg[[lg_col]],
        v[[actual_col]],
        use = "complete.obs"
      )
    )
  })
})

cat("=== Correlation with Next-Game FP by Category and Window ===\n")
cat_results |>
  select(window, category, r_own, r_opp, r_combined) |>
  mutate(across(starts_with("r_"), ~ round(.x, 3))) |>
  pivot_wider(
    names_from = category,
    values_from = c(r_own, r_opp, r_combined),
    names_sep = "_"
  ) |>
  print()
```

```{r marginal-week-value}
# How much does each additional week add to predictive power?
# Test windows from 1 to 20 games
marginal <- map_dfr(1:20, function(w) {
  gl <- game_log |>
    group_by(recent_team) |>
    mutate(
      roll_off = slide_dbl(lag(team_fp_total), mean, .before = w - 1, .complete = TRUE),
      roll_opp = slide_dbl(lag(def_allowed_fp_total), mean, .before = w - 1, .complete = TRUE)
    ) |>
    ungroup()

  # Get opponent's defensive rolling avg
  opp_lookup <- gl |> select(recent_team, season, week, roll_def = roll_opp)

  gl <- gl |>
    left_join(opp_lookup, by = c("opponent" = "recent_team", "season", "week"))

  v <- gl |> filter(!is.na(roll_off) & !is.na(roll_def))

  tibble(
    window = w,
    n = nrow(v),
    r_own = cor(v$roll_off, v$team_fp_total, use = "complete.obs"),
    r_opp = cor(v$roll_def, v$team_fp_total, use = "complete.obs"),
    r_combined = cor(v$roll_off - v$roll_def + league_avg$lg_total,
                     v$team_fp_total, use = "complete.obs"),
    r2_combined = r_combined^2
  )
})

cat("=== Marginal Value of Each Additional Week ===\n")
marginal |>
  mutate(
    marginal_r2 = r2_combined - lag(r2_combined),
    across(c(r_own, r_opp, r_combined, r2_combined, marginal_r2), ~ round(.x, 4))
  ) |>
  print(n = 20)
```

```{r marginal-plot}
marginal |>
  ggplot(aes(x = window, y = r2_combined)) +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_point(size = 2, color = "steelblue") +
  geom_vline(xintercept = c(5, 8, 10), linetype = "dashed", alpha = 0.3) +
  labs(
    title = "Predictive Power by Rolling Window Size",
    subtitle = "R-squared: rolling offensive avg + opponent defensive avg → next game total FP",
    x = "Number of prior games in rolling window",
    y = "R-squared"
  ) +
  scale_x_continuous(breaks = 1:20) +
  theme_minimal()
```

### Step 14: Model comparison — what actually predicts fantasy production?

Before building a model, we need to understand what information is valuable. We compare rolling averages (our stats-based approach) against Vegas implied totals (the market's prediction). Training on 2016-2024, testing on 2025.

```{r model-setup}
# Add features needed for modeling
game_log <- game_log |>
  mutate(
    is_home = as.integer(location == "home"),
    team_spread = if_else(location == "home", spread_line, -spread_line)
  )

# Filter to complete cases for fair model comparison
model_data <- game_log |>
  filter(!is.na(roll_8_off_total), !is.na(opp_roll_8_def_total), !is.na(implied_total))

train <- model_data |> filter(season <= 2024)
test <- model_data |> filter(season == 2025)

cat("Training:", nrow(train), "games (2016-2024)\n")
cat("Testing:", nrow(test), "games (2025)\n")
```

```{r model-comparison}
# Head-to-head: rolling averages vs Vegas vs combined
models <- list(
  "Rolling avg only"   = lm(team_fp_total ~ roll_8_off_total + opp_roll_8_def_total, data = train),
  "Vegas only"         = lm(team_fp_total ~ implied_total, data = train),
  "Vegas + home/away"  = lm(team_fp_total ~ implied_total + is_home, data = train),
  "Rolling + Vegas"    = lm(team_fp_total ~ roll_8_off_total + opp_roll_8_def_total + implied_total, data = train),
  "Full"               = lm(team_fp_total ~ roll_8_off_total + opp_roll_8_def_total + implied_total + is_home, data = train)
)

model_comparison <- map_dfr(names(models), function(name) {
  m <- models[[name]]
  preds <- predict(m, newdata = test)
  tibble(
    model = name,
    train_r2 = round(summary(m)$r.squared, 4),
    test_r2 = round(cor(preds, test$team_fp_total)^2, 4),
    test_mae = round(mean(abs(preds - test$team_fp_total)), 1)
  )
})

cat("=== Total FP Model Comparison ===\n\n")
print(model_comparison)

# Key finding: Vegas dominates. Show the full model coefficients to prove it.
cat("\nFull model coefficients (are rolling avgs significant after adding Vegas?):\n")
full_coefs <- summary(models[["Full"]])$coefficients
for (i in 1:nrow(full_coefs)) {
  sig <- ifelse(full_coefs[i, 4] < 0.001, "***",
         ifelse(full_coefs[i, 4] < 0.05, "*", ""))
  cat(sprintf("  %-25s  coef=%6.3f  p=%.3f %s\n",
    rownames(full_coefs)[i], full_coefs[i, 1], full_coefs[i, 4], sig))
}

cat("\nKey insight: implied_total alone explains ~19% of variance.\n")
cat("Rolling averages add nothing — Vegas already bakes in team quality and matchup.\n")
```

```{r vegas-calibration}
# Calibration plot: Vegas implied total vs actual FP
test$pred_vegas <- predict(models[["Vegas only"]], newdata = test)
test_r2 <- round(cor(test$pred_vegas, test$team_fp_total)^2, 3)
test_mae <- round(mean(abs(test$pred_vegas - test$team_fp_total)), 1)

test |>
  ggplot(aes(x = pred_vegas, y = team_fp_total)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  geom_smooth(method = "lm", se = FALSE, color = "steelblue") +
  labs(
    title = "Vegas Implied Total vs Actual Team FP (2025)",
    subtitle = paste("R² =", test_r2, "| MAE =", test_mae, "FP"),
    x = "Predicted FP (from implied total)",
    y = "Actual FP"
  ) +
  theme_minimal()
```

### Step 15: Category-level prediction — where rolling averages add value

Vegas lines predict total production well but don't tell us how production splits between passing, rushing, and receiving. This is where rolling averages become useful — not for "how much" but for "what kind" of production.

```{r category-model-comparison}
# For each category: compare Vegas-only vs Rolling-only vs Combined
cat("=== Category Models: Vegas vs Rolling vs Combined (test 2025) ===\n\n")

cat_comparison <- map_dfr(c("passing", "rushing", "receiving"), function(cat_name) {
  actual <- paste0("team_fp_", cat_name)
  off_col <- paste0("roll_8_off_", cat_name)
  opp_col <- paste0("opp_roll_8_def_", cat_name)

  m_vegas <- lm(as.formula(paste(actual, "~ implied_total")), data = train)
  m_roll  <- lm(as.formula(paste(actual, "~", off_col, "+", opp_col)), data = train)
  m_combo <- lm(as.formula(paste(actual, "~", off_col, "+", opp_col, "+ implied_total")), data = train)

  pv <- predict(m_vegas, newdata = test)
  pr <- predict(m_roll, newdata = test)
  pc <- predict(m_combo, newdata = test)

  # P-values for rolling avg features in combined model
  combo_coefs <- summary(m_combo)$coefficients

  tibble(
    category = cat_name,
    vegas_r2 = round(cor(pv, test[[actual]])^2, 4),
    rolling_r2 = round(cor(pr, test[[actual]])^2, 4),
    combined_r2 = round(cor(pc, test[[actual]])^2, 4),
    own_roll_p = round(combo_coefs[off_col, 4], 3),
    opp_def_p = round(combo_coefs[opp_col, 4], 3)
  )
})

print(cat_comparison)

cat("\nFindings:\n")
cat("  - Vegas dominates for passing and receiving (game total drives these)\n")
cat("  - Rolling avgs add most for RUSHING — team tendencies matter independently\n")
cat("  - Team's own rolling avg is always significant (p<0.001)\n")
cat("  - Opponent defensive rolling avg is significant only for rushing\n")
```

### Step 16: Two-step model — Vegas total x rolling category shares

The best approach separates two questions: (1) how many total FP will this team score? (answer: use Vegas) and (2) how will that production split across categories? (answer: use rolling averages). Multiply the two for category-level predictions.

```{r two-step-model}
# Compute rolling category shares
model_data <- model_data |>
  mutate(
    roll_8_total = roll_8_off_passing + roll_8_off_rushing + roll_8_off_receiving,
    roll_pass_share = roll_8_off_passing / roll_8_total,
    roll_rush_share = roll_8_off_rushing / roll_8_total,
    roll_recv_share = roll_8_off_receiving / roll_8_total
  )

train <- model_data |> filter(season <= 2024)
test <- model_data |> filter(season == 2025)

# Step 1: predict total FP from Vegas
m_total <- lm(team_fp_total ~ implied_total, data = train)
test$pred_total <- predict(m_total, newdata = test)

# Step 2: predict category shares from rolling avgs + opponent defense
# Step 3: multiply for category predictions
cat("=== Two-Step Model vs Direct Model (test 2025) ===\n\n")

two_step_results <- map_dfr(c("passing", "rushing", "receiving"), function(cat_name) {
  actual <- paste0("team_fp_", cat_name)
  off_col <- paste0("roll_8_off_", cat_name)
  opp_col <- paste0("opp_roll_8_def_", cat_name)

  share_name <- switch(cat_name,
    passing = "pass_share", rushing = "rush_share", receiving = "recv_share")
  roll_share <- switch(cat_name,
    passing = "roll_pass_share", rushing = "roll_rush_share", receiving = "roll_recv_share")

  # Actual shares for training
  train$actual_share <- train[[actual]] / train$team_fp_total
  test$actual_share <- test[[actual]] / test$team_fp_total

  # Two-step: predict share, multiply by predicted total
  m_share <- lm(as.formula(paste("actual_share ~", roll_share, "+", opp_col)), data = train)
  test$pred_share <- predict(m_share, newdata = test)
  pred_twostep <- test$pred_total * test$pred_share

  # Direct: single model with rolling avgs + Vegas
  m_direct <- lm(as.formula(paste(actual, "~", off_col, "+", opp_col, "+ implied_total")), data = train)
  pred_direct <- predict(m_direct, newdata = test)

  tibble(
    category = cat_name,
    direct_r2 = round(cor(pred_direct, test[[actual]])^2, 4),
    direct_mae = round(mean(abs(pred_direct - test[[actual]])), 1),
    twostep_r2 = round(cor(pred_twostep, test[[actual]])^2, 4),
    twostep_mae = round(mean(abs(pred_twostep - test[[actual]])), 1)
  )
})

print(two_step_results)

cat("\nThe two-step approach (Vegas total x rolling shares) slightly outperforms\n")
cat("the direct model across all categories. This decomposition is also more\n")
cat("interpretable: 'how much production' is separate from 'what kind'.\n")
```

```{r prediction-calibration}
# Final calibration: two-step predictions vs actuals for each category
# Using the best approach (two-step)
test$pred_pass <- test$pred_total * predict(
  lm(I(team_fp_passing / team_fp_total) ~ roll_pass_share + opp_roll_8_def_passing, data = train),
  newdata = test)
test$pred_rush <- test$pred_total * predict(
  lm(I(team_fp_rushing / team_fp_total) ~ roll_rush_share + opp_roll_8_def_rushing, data = train),
  newdata = test)
test$pred_recv <- test$pred_total * predict(
  lm(I(team_fp_receiving / team_fp_total) ~ roll_recv_share + opp_roll_8_def_receiving, data = train),
  newdata = test)

# Residual analysis by game environment
cat("=== Residual Analysis by Implied Total Bucket (2025) ===\n\n")
test |>
  filter(!is.na(implied_total)) |>
  mutate(
    resid_total = team_fp_total - pred_total,
    impl_bucket = cut(implied_total,
      breaks = c(0, 20, 23, 26, 35),
      labels = c("< 20", "20-23", "23-26", "26+"))
  ) |>
  group_by(impl_bucket) |>
  summarise(
    n = n(),
    mean_pred = round(mean(pred_total), 1),
    mean_actual = round(mean(team_fp_total), 1),
    mean_resid = round(mean(resid_total), 1),
    .groups = "drop"
  ) |>
  print()

# Validation: residuals should be roughly centered at 0 in each bucket
cat("\nResiduals near 0 across buckets = well-calibrated model.\n")
```

```{r summary-findings}
cat("===========================================================\n")
cat("  SUMMARY: Predicting Team Fantasy Production by Category   \n")
cat("===========================================================\n\n")

cat("1. TOTAL FP: Vegas implied total is the single best predictor (R² ≈ 0.19).\n")
cat("   Rolling averages add nothing once Vegas is included — the market\n")
cat("   already incorporates team quality, matchup, and context.\n\n")

cat("2. CATEGORY SPLITS: Rolling averages are useful here. Team tendencies\n")
cat("   (pass-heavy vs run-heavy) persist and Vegas doesn't capture them.\n")
cat("   - Receiving share is most predictable (teams have consistent pass/run ratios)\n")
cat("   - Rushing is where opponent defense matters most (p<0.001)\n\n")

cat("3. BEST APPROACH: Two-step model\n")
cat("   Step 1: Predict total FP from Vegas implied total\n")
cat("   Step 2: Predict category shares from 8-game rolling averages\n")
cat("   Step 3: Multiply total × share = category prediction\n\n")

cat("4. PRACTICAL LIMITS: Even the best model explains ~13% of category variance.\n")
cat("   Individual game outcomes are inherently noisy. The value is in identifying\n")
cat("   systematic edges — the other 87% is what makes DFS a game of skill + luck.\n\n")

cat("5. ROLLING WINDOW: 5-10 games is the sweet spot. Fewer is too noisy,\n")
cat("   more than ~16 starts capturing stale information.\n")
```

## Draft Analysis Ideas

```{r draft-analysis}
# Example: pick duration analysis (slow vs fast drafters)
# Example: positional run detection
# Example: value over ADP by round

# Your code here...
```

---

```{r cleanup, include=FALSE}
dbDisconnect(con)
```

