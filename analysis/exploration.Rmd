---
title: "NFL-DB Exploration"
subtitle: "Prototyping calculated fields for players and teams"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(DBI)
library(RPostgres)
library(dplyr)
library(tidyverse)
library(dotenv)
library(nflverse)

# Load .env from project root
load_dot_env(file = here::here(".env"))

# Connect to Supabase Postgres
con <- dbConnect(
  Postgres(),
  host     = "db.twfzcrodldvhpfaykasj.supabase.co",
  port     = 5432,
  dbname   = "postgres",
  user     = "postgres",
  password = Sys.getenv("SUPABASE_DB_PASSWORD"),
  sslmode  = "require"
)
```

# Database Overview

```{r tables}
# List all public tables with row counts
dbGetQuery(con, "
  SELECT relname as table_name, n_live_tup as row_count
  FROM pg_stat_user_tables
  WHERE schemaname = 'public'
  ORDER BY n_live_tup DESC
")
```

# Players

```{r players}
players <- dbGetQuery(con, "SELECT * FROM players")
glimpse(players)
```

```{r player-positions}
players %>%
  count(position, sort = TRUE) %>%
  mutate(pct = round(n / sum(n) * 100, 1))
```

# Draft Picks

```{r draft-picks}
picks <- dbGetQuery(con, "
  SELECT dp.*, p.first_name, p.last_name, p.position
  FROM draft_picks dp
  JOIN players p ON dp.player_id = p.player_id
")
glimpse(picks)
```

# ADP

```{r adp}
adp <- dbGetQuery(con, "SELECT * FROM adp")
glimpse(adp)
```

# Player Seasons

```{r player-seasons}
player_seasons <- dbGetQuery(con, "SELECT * FROM player_seasons")
glimpse(player_seasons)
```

---

# Sandbox: Calculated Fields

Use the sections below to prototype calculated fields. Write your exploratory
code here, and when you're happy with the logic, ask Claude Code to read this
file and translate it to Python / SQL migrations.

## Player-Level Ideas

```{r player-calcs}
# Example: career draft capital (years * round * pick)
# Example: ADP trend across years
# Example: positional scarcity metrics

# Your code here...
```

## Team-Level Ideas

I will eventually use this database in my weekly daily fantasy content creation. Much of my weekly 
daily fantasy analysis comes down to projecting team level production with a very intense focus on variance. 
In these large daily fantasy contests, payouts are very top heavy, which means the focus is on chasing outlier type scenarios, both in terms of players hitting for larger than expected shares of their team's fantasy production and in terms of teams outperforming expectations. 

One of the ways I have done this in the past is to calculate offensive production numbers each week in each specific offensive category -- rushing fantasy points, passing fantasy points, receiving fantasy points. Then focus in on how much those numbers vary week to week. I do the same for each defense in terms of what they allow each week to opposing offenses. How those the offense and defensive production metrics interact both in terms of median/mean and variance is very important to me. Sample size is also a big deal, since there is so much turnover from one season to the next. Indicators that are strong late in the season are less so early in the year. Thus, I am interested in how much of a signal there is to be taken from previous seasons. Did the performance of a defense in 2024 give us any clues about how they were likely to perform early in 2025?

I am also interested in factors like each team's implied team total, how much they are favored by, the overall game total, and how those things impacted mean, median, and variance in the categories of interest.

I am interested in these numbers at both a team level and a positional level (QB, RB, WR, TE)

Within the nflreadr package in r, there are some very useful functions. For now, I want to just note those here. The first is load_ff_opportunity. Here are the columns that returns (for the current season):

```{r team-calcs}

load_ff_opportunity() |> 
  colnames()
```

game_id is important because it can tie back to the load_schedule function that contains the pre-game lines and over unders (which we can use to calculate implied team totals). We can also use this to figure out who the defense was in each game, which we need to calculate all of these stats for points against

posteam is the NFL team on offense (in possession of the ball). For now, I am only mildely interested in the expected metrics, but it may be worth calculating them for each category in case I want to explore it more in the future. The main thing here is pass fantasy points per team, rushing fantasy points per team, etc. I would like to have these for standard scoring, full PPR scoring, and half PPR scoring. We may just want to isolate the receptions column and then figure out standard fantasy points for each and go from there. 

Here is another function we could use to get these same stats (minus the expected numbers). It may or not be the better option. It does have fantasy_points and fantasy_points_ppr (fantasy points half ppr can be calculated as the mean between the two). 

```{r}
load_player_stats() |>
colnames()
```

### Step 1: Pull the raw data

We use `load_player_stats()` as our primary source because it already has `fantasy_points` (standard) and `fantasy_points_ppr`, plus all the component stats we need. We pull a full decade (2015-2025) to give us enough history for year-over-year signal analysis. We also pull `load_schedules()` to get the pre-game lines (spread, over/under) which we'll use to calculate implied team totals.

```{r pull-raw-data, cache=TRUE}
seasons <- 2015:2025

# Weekly player-level offensive stats
# Rename 'team' to 'recent_team' for consistency across all our joins
player_stats <- load_player_stats(seasons = seasons, stat_type = "offense") |>
  rename(recent_team = team)

# Game schedules with betting lines
schedules <- load_schedules(seasons = seasons)
```

### Step 2: Calculate fantasy point components by category

The key insight for DFS analysis is breaking total fantasy points into **passing**, **rushing**, and **receiving** components. This lets us see which category is driving a team's production and how volatile each category is independently. We also calculate half-PPR as the midpoint between standard and full PPR.

Standard scoring assumptions: pass TD = 4pt, rush/rec TD = 6pt, pass yd = 0.04pt/yd, rush/rec yd = 0.1pt/yd, INT = -2pt, fumble lost = -2pt.

Note for claude: It's a small thing, but don't forget two point conversions which are two points each for passing, rushing, and receiving. 

```{r fp-components}
player_stats <- player_stats |>
  mutate(
    # Half PPR total (midpoint of standard and full PPR)
    fantasy_points_hppr = (fantasy_points + fantasy_points_ppr) / 2,

    # Standard scoring components by category
    # Includes 2pt conversions (2pts each) and fumbles lost (-2pts each)
    fp_passing = passing_yards * 0.04 + passing_tds * 4 - passing_interceptions * 2
                 + passing_2pt_conversions * 2 - sack_fumbles_lost * 2,
    fp_rushing = rushing_yards * 0.1 + rushing_tds * 6
                 + rushing_2pt_conversions * 2 - rushing_fumbles_lost * 2,
    fp_receiving = receiving_yards * 0.1 + receiving_tds * 6
                   + receiving_2pt_conversions * 2 - receiving_fumbles_lost * 2,

    # Receiving components for PPR/half-PPR (add reception bonus)
    fp_receiving_ppr = fp_receiving + receptions * 1.0,
    fp_receiving_hppr = fp_receiving + receptions * 0.5
  )
```

### Step 3: Aggregate to team-week level

This is the core table -- one row per team per week with total fantasy points broken out by category. This is what we'll use to calculate means, variance, and everything else. We filter to regular season only (weeks 1-18) since that's what matters for DFS.

```{r team-week-agg}
team_week <- player_stats |>
  filter(week <= 18) |>
  group_by(recent_team, season, week) |>
  summarise(
    # Standard scoring by category
    team_fp_passing = sum(fp_passing, na.rm = TRUE),
    team_fp_rushing = sum(fp_rushing, na.rm = TRUE),
    team_fp_receiving = sum(fp_receiving, na.rm = TRUE),
    team_fp_total = sum(fantasy_points, na.rm = TRUE),

    # Full PPR
    team_fp_receiving_ppr = sum(fp_receiving_ppr, na.rm = TRUE),
    team_fp_total_ppr = sum(fantasy_points_ppr, na.rm = TRUE),

    # Half PPR
    team_fp_receiving_hppr = sum(fp_receiving_hppr, na.rm = TRUE),
    team_fp_total_hppr = sum(fantasy_points_hppr, na.rm = TRUE),
    .groups = "drop"
  )

glimpse(team_week)
```

### Step 4: Add game context (lines, implied totals)

For DFS, game environment matters enormously. A team's implied team total (derived from the spread and over/under) is the market's best guess at how many points they'll score. We want to know: when a team is in a high-total game or is heavily favored, how does that affect the distribution of their fantasy production?

We use nflreadr's `clean_homeaway()` to properly pivot the schedule into one-row-per-team format. The `spread_line` column represents how many points the home team is favored by (positive = home favored). We use that with `location` to compute each team's implied total.

Implied team total formula: `implied = (total + spread_from_team_perspective) / 2`. For example, if KC is home and `spread_line = 3` (KC favored by 3) in a game with total 46: KC implied = (46 + 3) / 2 = 24.5, BAL implied = (46 - 3) / 2 = 21.5.

```{r game-context}
# Use clean_homeaway() to pivot home/away into one-row-per-team
# spread_line: positive = home team favored (e.g., 3 means home -3)
game_context <- schedules |>
  filter(game_type == "REG") |>
  select(game_id, season, week, home_team, away_team,
         spread_line, total_line, home_score, away_score) |>
  clean_homeaway() |>
  mutate(
    # Implied team total using spread_line (positive = home favored)
    # Home team gets the boost, away team gets the reduction
    implied_total = (total_line + if_else(location == "home", spread_line, -spread_line)) / 2
  ) |>
  rename(actual_score = team_score, opp_score = opponent_score)

# Sanity check: week 1 2024 KC (home favorite by 3) vs BAL (away underdog)
game_context |>
  filter(season == 2024, week == 1, team %in% c("KC", "BAL")) |>
  select(team, opponent, location, spread_line, total_line, implied_total,
         actual_score, opp_score)

# Join context onto our team-week fantasy production
team_week <- team_week |>
  left_join(
    game_context |> select(season, week, team, opponent, location,
                           spread_line, total_line, implied_total,
                           actual_score, opp_score),
    by = c("recent_team" = "team", "season", "week")
  )

# Quick sanity check: how many rows have lines?
team_week |>
  summarise(
    total_rows = n(),
    has_lines = sum(!is.na(implied_total)),
    pct_with_lines = round(has_lines / total_rows * 100, 1)
  )
```

### Step 5: Defensive points allowed

This mirrors the offensive table but from the defense's perspective. Each defense's "points allowed" in a category is simply what the opposing offense scored in that category. This is critical for DFS -- we need to know not just how good an offense is, but how much the opposing defense gives up (and how variable that is).

```{r defensive-allowed}
# Create a defensive version: what did each team ALLOW?
# The opponent's offensive production = this team's defensive allowance
team_week_def <- team_week |>
  select(season, week, recent_team, opponent,
         starts_with("team_fp_")) |>
  rename_with(
    ~ str_replace(.x, "team_fp_", "def_allowed_fp_"),
    starts_with("team_fp_")
  )

# Join so each row now has BOTH:
# - This team's offensive production (team_fp_*)
# - What the opposing defense allowed (def_allowed_fp_*)
team_week <- team_week |>
  left_join(
    team_week_def |> select(-opponent),
    by = c("opponent" = "recent_team", "season", "week")
  )
```

### Step 6: Season-level summary with means and variance

This is where it gets interesting for DFS. For each team-season we calculate the mean and standard deviation of each category. The coefficient of variation (CV = SD/mean) normalizes the variance so we can compare across teams with different production levels. High CV = boom/bust = potentially interesting for tournaments.

```{r season-summary}
team_season_summary <- team_week |>
  group_by(recent_team, season) |>
  summarise(
    games = n(),

    # Offensive means (standard scoring)
    mean_fp_passing = mean(team_fp_passing),
    mean_fp_rushing = mean(team_fp_rushing),
    mean_fp_receiving = mean(team_fp_receiving),
    mean_fp_total = mean(team_fp_total),

    # Offensive standard deviations
    sd_fp_passing = sd(team_fp_passing),
    sd_fp_rushing = sd(team_fp_rushing),
    sd_fp_receiving = sd(team_fp_receiving),
    sd_fp_total = sd(team_fp_total),

    # Coefficient of variation (high = more boom/bust)
    cv_fp_passing = sd_fp_passing / mean_fp_passing,
    cv_fp_rushing = sd_fp_rushing / mean_fp_rushing,
    cv_fp_receiving = sd_fp_receiving / mean_fp_receiving,
    cv_fp_total = sd_fp_total / mean_fp_total,

    # Defensive means (what this team allowed)
    mean_def_passing = mean(def_allowed_fp_passing, na.rm = TRUE),
    mean_def_rushing = mean(def_allowed_fp_rushing, na.rm = TRUE),
    mean_def_receiving = mean(def_allowed_fp_receiving, na.rm = TRUE),
    mean_def_total = mean(def_allowed_fp_total, na.rm = TRUE),

    # Defensive standard deviations
    sd_def_passing = sd(def_allowed_fp_passing, na.rm = TRUE),
    sd_def_rushing = sd(def_allowed_fp_rushing, na.rm = TRUE),
    sd_def_receiving = sd(def_allowed_fp_receiving, na.rm = TRUE),
    sd_def_total = sd(def_allowed_fp_total, na.rm = TRUE),

    .groups = "drop"
  )

# Top offenses in 2024 by total FP with variance
team_season_summary |>
  filter(season == 2024) |>
  arrange(desc(mean_fp_total)) |>
  select(recent_team, games, mean_fp_total, sd_fp_total, cv_fp_total,
         mean_fp_passing, mean_fp_rushing, mean_fp_receiving) |>
  head(10)
```

```{r worst-defenses-2024}
# Worst defenses in 2024 (most FP allowed = best matchups for DFS)
team_season_summary |>
  filter(season == 2024) |>
  arrange(desc(mean_def_total)) |>
  select(recent_team, mean_def_total, sd_def_total,
         mean_def_passing, mean_def_rushing, mean_def_receiving) |>
  head(10)
```

### Step 7: Positional breakdown (QB, RB, WR, TE per team-week)

For DFS stacking, we need to know not just total team production but how it distributes across positions. A team that funnels production through WRs has a different stacking profile than one that's RB-heavy. We also calculate each position's share of total team FP.

```{r positional-breakdown}
pos_team_week <- player_stats |>
  filter(week <= 18, position_group %in% c("QB", "RB", "WR", "TE")) |>
  group_by(recent_team, season, week, position_group) |>
  summarise(
    pos_fp = sum(fantasy_points, na.rm = TRUE),
    pos_fp_ppr = sum(fantasy_points_ppr, na.rm = TRUE),
    pos_fp_hppr = sum(fantasy_points_hppr, na.rm = TRUE),
    .groups = "drop"
  )

# Add total team production and calculate position share
pos_team_week <- pos_team_week |>
  left_join(
    team_week |> select(recent_team, season, week, team_fp_total),
    by = c("recent_team", "season", "week")
  ) |>
  mutate(pos_share = pos_fp / team_fp_total)

# Season-level positional summary
pos_season <- pos_team_week |>
  group_by(recent_team, season, position_group) |>
  summarise(
    mean_pos_fp = mean(pos_fp),
    sd_pos_fp = sd(pos_fp),
    cv_pos_fp = sd_pos_fp / mean_pos_fp,
    mean_pos_share = mean(pos_share),
    .groups = "drop"
  )

# 2024 positional shares by team (which teams funnel to which positions?)
pos_season |>
  filter(season == 2024) |>
  select(recent_team, position_group, mean_pos_fp, mean_pos_share) |>
  pivot_wider(
    names_from = position_group,
    values_from = c(mean_pos_fp, mean_pos_share),
    names_sep = "_"
  ) |>
  arrange(desc(mean_pos_fp_WR))
```

### Step 8: Game environment impact on production

How does implied total / spread affect production and variance? This is key for DFS -- when Vegas expects a shootout, does that actually translate to higher (and more variable) fantasy production?

```{r game-environment}
# Bucket games by implied total
team_week_bucketed <- team_week |>
  filter(!is.na(implied_total)) |>
  mutate(
    implied_bucket = cut(implied_total,
      breaks = c(0, 18, 21, 24, 27, 35),
      labels = c("< 18", "18-21", "21-24", "24-27", "27+")
    ),
    # Team spread: positive = favored (home gets spread_line, away gets negated)
    team_spread = if_else(location == "home", spread_line, -spread_line),
    spread_bucket = cut(team_spread,
      breaks = c(-Inf, -7, -3, 0, 3, 7, Inf),
      labels = c("Dog 7+", "Dog 3-7", "Dog 0-3",
                 "Fav 0-3", "Fav 3-7", "Fav 7+")
    )
  )

# How does implied total affect production?
team_week_bucketed |>
  group_by(implied_bucket) |>
  summarise(
    n_games = n(),
    mean_total = mean(team_fp_total),
    sd_total = sd(team_fp_total),
    mean_passing = mean(team_fp_passing),
    mean_rushing = mean(team_fp_rushing),
    mean_receiving = mean(team_fp_receiving),
    .groups = "drop"
  )
```

```{r spread-impact}
# How does spread affect production?
team_week_bucketed |>
  group_by(spread_bucket) |>
  summarise(
    n_games = n(),
    mean_total = mean(team_fp_total),
    sd_total = sd(team_fp_total),
    mean_passing = mean(team_fp_passing),
    mean_rushing = mean(team_fp_rushing),
    .groups = "drop"
  )
```

### Step 9: Year-over-year signal (does last season predict this season?)

This is the sample size question. If a defense was terrible in 2024, should we expect them to still be bad in weeks 1-4 of 2025? We test this by correlating full-season metrics with early-season performance the following year. High correlation = more predictive signal we can use early in the season.

```{r yoy-signal}
# Compare: full season N performance vs first 4 weeks of season N+1
early_season <- team_week |>
  filter(week <= 4) |>
  group_by(recent_team, season) |>
  summarise(
    early_off_total = mean(team_fp_total),
    early_def_total = mean(def_allowed_fp_total, na.rm = TRUE),
    .groups = "drop"
  )

yoy <- team_season_summary |>
  select(recent_team, season,
         mean_fp_total, mean_def_total) |>
  inner_join(
    early_season |> mutate(season = season - 1),
    by = c("recent_team", "season")
  )

# Offense: does prior year full season predict next year weeks 1-4?
cat("Offense YoY correlation (full season → next year wk 1-4):",
    round(cor(yoy$mean_fp_total, yoy$early_off_total, use = "complete.obs"), 3), "\n")

# Defense: does prior year full season predict next year weeks 1-4?
cat("Defense YoY correlation (full season → next year wk 1-4):",
    round(cor(yoy$mean_def_total, yoy$early_def_total, use = "complete.obs"), 3), "\n")
```

```{r yoy-weekly-decay}
# How does the signal decay? Check correlation at different week cutoffs.
# This tells us: at what point in the new season does current-year data
# become more useful than last year's data?
yoy_by_week <- map_dfr(1:17, function(wk) {
  early <- team_week |>
    filter(week <= wk) |>
    group_by(recent_team, season) |>
    summarise(
      early_off = mean(team_fp_total),
      early_def = mean(def_allowed_fp_total, na.rm = TRUE),
      .groups = "drop"
    ) |>
    mutate(season = season - 1)

  joined <- team_season_summary |>
    select(recent_team, season, mean_fp_total, mean_def_total) |>
    inner_join(early, by = c("recent_team", "season"))

  tibble(
    through_week = wk,
    off_cor = cor(joined$mean_fp_total, joined$early_off, use = "complete.obs"),
    def_cor = cor(joined$mean_def_total, joined$early_def, use = "complete.obs")
  )
})

# Plot: when does prior year signal fade?
yoy_by_week |>
  pivot_longer(cols = c(off_cor, def_cor), names_to = "side", values_to = "correlation") |>
  ggplot(aes(x = through_week, y = correlation, color = side)) +
  geom_line(linewidth = 1) +
  geom_point() +
  labs(
    title = "Prior Year Signal Decay",
    subtitle = "Correlation between full prior season and current season through week N",
    x = "Current season through week...",
    y = "Correlation with prior year full season"
  ) +
  scale_color_manual(values = c("off_cor" = "steelblue", "def_cor" = "firebrick"),
                     labels = c("off_cor" = "Offense", "def_cor" = "Defense")) +
  theme_minimal()
```


### Step 10: Predict 2026 team-level baselines

Using the year-over-year correlations from Step 9, we can project 2026 offensive and defensive fantasy point baselines for each team. The method is simple regression toward the mean: a team's projected 2026 value is a weighted blend of their 2025 performance and the league average, where the weight is the YoY correlation.

Formula: `projected = league_mean + r * (team_2025 - league_mean)` where `r` is the YoY correlation.

This means: if offense has r=0.40, a team that was 10 FP above average in 2025 is projected to be ~4 FP above average in 2026. Defense (r=0.26) regresses even harder — 10 FP above average becomes ~2.6 FP above average.

```{r predict-2026}
# Get YoY correlations (full season N → full season N+1)
yoy_full <- team_season_summary |>
  inner_join(
    team_season_summary |>
      mutate(season = season - 1) |>
      rename(next_off = mean_fp_total, next_def = mean_def_total),
    by = c("recent_team", "season")
  )

r_offense <- cor(yoy_full$mean_fp_total, yoy_full$next_off, use = "complete.obs")
r_defense <- cor(yoy_full$mean_def_total, yoy_full$next_def, use = "complete.obs")

cat("YoY correlations used for regression:\n")
cat("  Offense:", round(r_offense, 3), "\n")
cat("  Defense:", round(r_defense, 3), "\n\n")

# 2025 team averages + league means
team_2025 <- team_season_summary |>
  filter(season == 2025)

league_mean_off <- mean(team_2025$mean_fp_total)
league_mean_def <- mean(team_2025$mean_def_total)

cat("2025 league averages:\n")
cat("  Offense:", round(league_mean_off, 1), "FP/game\n")
cat("  Defense:", round(league_mean_def, 1), "FP allowed/game\n\n")

# Project 2026: regress toward league mean
projections_2026 <- team_2025 |>
  mutate(
    proj_off_2026 = league_mean_off + r_offense * (mean_fp_total - league_mean_off),
    proj_def_2026 = league_mean_def + r_defense * (mean_def_total - league_mean_def),
    # How far above/below average is each projection?
    off_edge = proj_off_2026 - league_mean_off,
    def_edge = proj_def_2026 - league_mean_def
  ) |>
  select(recent_team, mean_fp_total, proj_off_2026, off_edge,
         mean_def_total, proj_def_2026, def_edge) |>
  arrange(desc(proj_off_2026))

cat("=== 2026 Projected Offensive FP/game (sorted best to worst) ===\n")
projections_2026 |>
  select(recent_team, `2025 actual` = mean_fp_total,
         `2026 proj` = proj_off_2026, `edge` = off_edge) |>
  mutate(across(where(is.numeric), ~ round(.x, 1))) |>
  print(n = 32)

cat("\n=== 2026 Projected Defensive FP Allowed/game (sorted worst to best) ===\n")
projections_2026 |>
  arrange(desc(proj_def_2026)) |>
  select(recent_team, `2025 actual` = mean_def_total,
         `2026 proj` = proj_def_2026, `edge` = def_edge) |>
  mutate(across(where(is.numeric), ~ round(.x, 1))) |>
  print(n = 32)
```

```{r predict-2026-category}
# Same approach but by fantasy point category (passing, rushing, receiving)
# First get category-level season summaries and YoY correlations
team_season_cats <- team_week |>
  group_by(recent_team, season) |>
  summarise(
    mean_passing = mean(team_fp_passing),
    mean_rushing = mean(team_fp_rushing),
    mean_receiving = mean(team_fp_receiving),
    mean_def_passing = mean(def_allowed_fp_passing, na.rm = TRUE),
    mean_def_rushing = mean(def_allowed_fp_rushing, na.rm = TRUE),
    mean_def_receiving = mean(def_allowed_fp_receiving, na.rm = TRUE),
    .groups = "drop"
  )

# Category YoY correlations
yoy_cats <- team_season_cats |>
  inner_join(
    team_season_cats |> mutate(season = season - 1) |>
      rename_with(~ paste0("next_", .x), -c(recent_team, season)),
    by = c("recent_team", "season")
  )

cat("=== Category-Level YoY Correlations ===\n")
cat("Offense:\n")
cat("  Passing: ", round(cor(yoy_cats$mean_passing, yoy_cats$next_mean_passing, use="complete.obs"), 3), "\n")
cat("  Rushing: ", round(cor(yoy_cats$mean_rushing, yoy_cats$next_mean_rushing, use="complete.obs"), 3), "\n")
cat("  Receiving:", round(cor(yoy_cats$mean_receiving, yoy_cats$next_mean_receiving, use="complete.obs"), 3), "\n")
cat("Defense:\n")
cat("  Pass allowed: ", round(cor(yoy_cats$mean_def_passing, yoy_cats$next_mean_def_passing, use="complete.obs"), 3), "\n")
cat("  Rush allowed: ", round(cor(yoy_cats$mean_def_rushing, yoy_cats$next_mean_def_rushing, use="complete.obs"), 3), "\n")
cat("  Rec allowed:  ", round(cor(yoy_cats$mean_def_receiving, yoy_cats$next_mean_def_receiving, use="complete.obs"), 3), "\n")
```

## Deeper Exploration: Rolling Averages, Opponent Adjustment, and Prediction

The goal of this section is to build toward a model that predicts team-level fantasy points by category for an upcoming game. The key ingredients:

1. **Rolling averages** — How has this team performed over their last N games? (across seasons)
2. **Opponent adjustment** — How much of a team's output was "them" vs "the matchup"?
3. **Window size analysis** — How many prior games give us the best signal?
4. **Prediction model** — Combine offensive rolling avg + opponent defensive rolling avg to predict next game

### Step 11: Create chronological game log with rolling averages

First we need a single chronological game log per team that spans seasons. This lets us compute rolling averages that carry over from late 2024 into early 2025, etc. We compute rolling averages at multiple window sizes (3, 5, 8, 10, 16 games) for each category.

```{r rolling-setup}
library(slider)  # for slide_dbl (rolling window functions)

# Build a chronological game log per team
# team_week already has season, week, recent_team, and all the FP columns
game_log <- team_week |>
  filter(!is.na(opponent)) |>
  arrange(recent_team, season, week) |>
  group_by(recent_team) |>
  mutate(game_num = row_number()) |>
  ungroup()

cat("Game log:", nrow(game_log), "team-games across",
    n_distinct(game_log$season), "seasons\n")
cat("Games per team: min =", min(table(game_log$recent_team)),
    "max =", max(table(game_log$recent_team)), "\n")

# Validation: each team should have ~17-18 games per season
game_log |>
  count(recent_team, season) |>
  summarise(min_games = min(n), max_games = max(n), mean_games = round(mean(n), 1))
```

```{r rolling-averages}
# Compute PRIOR rolling averages (exclude current game — we're predicting it)
# slide_dbl with .before = N-1, .complete = TRUE gives us N-game lookback
windows <- c(3, 5, 8, 10, 16)

# Offensive rolling averages by category
for (w in windows) {
  game_log <- game_log |>
    group_by(recent_team) |>
    mutate(
      "roll_{w}_off_total" := slide_dbl(lag(team_fp_total), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_off_passing" := slide_dbl(lag(team_fp_passing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_off_rushing" := slide_dbl(lag(team_fp_rushing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_off_receiving" := slide_dbl(lag(team_fp_receiving), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_total" := slide_dbl(lag(def_allowed_fp_total), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_passing" := slide_dbl(lag(def_allowed_fp_passing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_rushing" := slide_dbl(lag(def_allowed_fp_rushing), mean, .before = w - 1, .complete = TRUE),
      "roll_{w}_def_receiving" := slide_dbl(lag(def_allowed_fp_receiving), mean, .before = w - 1, .complete = TRUE)
    ) |>
    ungroup()
}

# Validation: spot check — KC's 5-game rolling offensive total going into week 6, 2024
kc_check <- game_log |>
  filter(recent_team == "KC", season == 2024, week %in% 1:6) |>
  select(season, week, team_fp_total, roll_5_off_total)
cat("\nValidation: KC 2024 weeks 1-6\n")
cat("Weeks 1-5 actual FP:", paste(round(kc_check$team_fp_total[1:5], 1), collapse = ", "), "\n")
cat("Mean of weeks 1-5:", round(mean(kc_check$team_fp_total[1:5]), 1), "\n")
cat("roll_5_off_total at week 6:", round(kc_check$roll_5_off_total[6], 1), "\n")
```

### Step 12: Opponent-adjusted metrics

Raw production is misleading — scoring 80 FP against the league's worst defense means less than scoring 65 against the best. We adjust by comparing each game to the opponent's rolling average:

`adjusted = actual - opponent_rolling_def_avg + league_avg`

This normalizes for schedule strength. A team that consistently beats expectation has positive adjustment; one inflated by weak opponents regresses.

```{r opponent-adjusted}
# We need the OPPONENT's rolling defensive average for each game
# Join opponent's rolling def stats onto each game
# Using 8-game window as our primary (will test others later)

# First, create opponent lookup: for each team-game, what was the opponent's
# rolling defensive average going INTO that game?
opp_def_lookup <- game_log |>
  select(recent_team, season, week,
         starts_with("roll_")) |>
  select(recent_team, season, week, matches("roll_.*_def_"))

# Join: for each row, get the OPPONENT's rolling defensive averages
game_log <- game_log |>
  left_join(
    opp_def_lookup |> rename_with(~ str_replace(.x, "roll_", "opp_roll_"), starts_with("roll_")),
    by = c("opponent" = "recent_team", "season", "week")
  )

# Calculate league average FP per game (for normalization baseline)
league_avg <- game_log |>
  summarise(
    lg_total = mean(team_fp_total),
    lg_passing = mean(team_fp_passing),
    lg_rushing = mean(team_fp_rushing),
    lg_receiving = mean(team_fp_receiving)
  )

cat("League averages (FP/game):\n")
cat("  Total:", round(league_avg$lg_total, 1), "\n")
cat("  Passing:", round(league_avg$lg_passing, 1), "\n")
cat("  Rushing:", round(league_avg$lg_rushing, 1), "\n")
cat("  Receiving:", round(league_avg$lg_receiving, 1), "\n")

# Opponent-adjusted offensive production (using 8-game opponent rolling avg)
# adjusted = actual - opp_def_rolling_avg + league_avg
# Interpretation: "what would this team have scored against an average defense?"
game_log <- game_log |>
  mutate(
    adj_off_total = team_fp_total - opp_roll_8_def_total + league_avg$lg_total,
    adj_off_passing = team_fp_passing - opp_roll_8_def_passing + league_avg$lg_passing,
    adj_off_rushing = team_fp_rushing - opp_roll_8_def_rushing + league_avg$lg_rushing,
    adj_off_receiving = team_fp_receiving - opp_roll_8_def_receiving + league_avg$lg_receiving
  )

# Validation: compare raw vs adjusted for a team with easy/hard schedule
cat("\n2024 season averages — raw vs opponent-adjusted (total FP):\n")
game_log |>
  filter(season == 2024, !is.na(adj_off_total)) |>
  group_by(recent_team) |>
  summarise(
    raw_mean = mean(team_fp_total),
    adj_mean = mean(adj_off_total),
    diff = adj_mean - raw_mean,
    .groups = "drop"
  ) |>
  arrange(desc(abs(diff))) |>
  mutate(across(where(is.numeric), ~ round(.x, 1))) |>
  head(10) |>
  print()
```

### Step 13: Predictive power by window size

The key question: how well do rolling averages predict the NEXT game? We test each window size and compare raw vs opponent-adjusted. We also look at how much each additional week of data improves our prediction.

```{r predictive-power}
# For each window size, correlate rolling average with actual next-game production
# We already have rolling averages computed; actual production is in the same row
# (rolling avg is PRIOR games, actual is current game)

results <- map_dfr(windows, function(w) {
  col_off <- paste0("roll_", w, "_off_total")
  col_def <- paste0("roll_", w, "_def_total")

  valid <- game_log |> filter(!is.na(.data[[col_off]]) & !is.na(.data[[col_def]]))

  tibble(
    window = w,
    n_games = nrow(valid),
    # How well does team's own rolling avg predict their next game?
    r_own_off = cor(valid[[col_off]], valid$team_fp_total, use = "complete.obs"),
    # How well does opponent's rolling def avg predict what team scores?
    r_opp_def = cor(valid[[paste0("opp_roll_", w, "_def_total")]],
                    valid$team_fp_total, use = "complete.obs"),
    # Combined: own offense + opponent defense
    r_combined = cor(
      valid[[col_off]] - valid[[paste0("opp_roll_", w, "_def_total")]] + league_avg$lg_total,
      valid$team_fp_total,
      use = "complete.obs"
    )
  )
})

cat("=== Correlation with Next-Game Total FP by Window Size ===\n")
results |>
  mutate(across(starts_with("r_"), ~ round(.x, 3))) |>
  print()

# Validation: r_own_off should increase with window size (more data = more signal)
# r_combined should be higher than either individual predictor
cat("\nValidation checks:\n")
cat("  Own off r increases with window? ",
    all(diff(results$r_own_off) >= -0.01), "\n")  # allow tiny wobble
cat("  Combined > own off alone? ",
    all(results$r_combined > results$r_own_off), "\n")
```

```{r predictive-by-category}
# Same analysis but broken out by category
cat_results <- map_dfr(windows, function(w) {
  valid <- game_log |>
    filter(!is.na(.data[[paste0("roll_", w, "_off_total")]]))

  cats <- c("passing", "rushing", "receiving")
  map_dfr(cats, function(cat) {
    off_col <- paste0("roll_", w, "_off_", cat)
    opp_col <- paste0("opp_roll_", w, "_def_", cat)
    actual_col <- paste0("team_fp_", cat)
    lg_col <- paste0("lg_", cat)

    v <- valid |> filter(!is.na(.data[[off_col]]) & !is.na(.data[[opp_col]]))

    tibble(
      window = w,
      category = cat,
      n = nrow(v),
      r_own = cor(v[[off_col]], v[[actual_col]], use = "complete.obs"),
      r_opp = cor(v[[opp_col]], v[[actual_col]], use = "complete.obs"),
      r_combined = cor(
        v[[off_col]] - v[[opp_col]] + league_avg[[lg_col]],
        v[[actual_col]],
        use = "complete.obs"
      )
    )
  })
})

cat("=== Correlation with Next-Game FP by Category and Window ===\n")
cat_results |>
  select(window, category, r_own, r_opp, r_combined) |>
  mutate(across(starts_with("r_"), ~ round(.x, 3))) |>
  pivot_wider(
    names_from = category,
    values_from = c(r_own, r_opp, r_combined),
    names_sep = "_"
  ) |>
  print()
```

```{r marginal-week-value}
# How much does each additional week add to predictive power?
# Test windows from 1 to 20 games
marginal <- map_dfr(1:20, function(w) {
  gl <- game_log |>
    group_by(recent_team) |>
    mutate(
      roll_off = slide_dbl(lag(team_fp_total), mean, .before = w - 1, .complete = TRUE),
      roll_opp = slide_dbl(lag(def_allowed_fp_total), mean, .before = w - 1, .complete = TRUE)
    ) |>
    ungroup()

  # Get opponent's defensive rolling avg
  opp_lookup <- gl |> select(recent_team, season, week, roll_def = roll_opp)

  gl <- gl |>
    left_join(opp_lookup, by = c("opponent" = "recent_team", "season", "week"))

  v <- gl |> filter(!is.na(roll_off) & !is.na(roll_def))

  tibble(
    window = w,
    n = nrow(v),
    r_own = cor(v$roll_off, v$team_fp_total, use = "complete.obs"),
    r_opp = cor(v$roll_def, v$team_fp_total, use = "complete.obs"),
    r_combined = cor(v$roll_off - v$roll_def + league_avg$lg_total,
                     v$team_fp_total, use = "complete.obs"),
    r2_combined = r_combined^2
  )
})

cat("=== Marginal Value of Each Additional Week ===\n")
marginal |>
  mutate(
    marginal_r2 = r2_combined - lag(r2_combined),
    across(c(r_own, r_opp, r_combined, r2_combined, marginal_r2), ~ round(.x, 4))
  ) |>
  print(n = 20)
```

```{r marginal-plot}
marginal |>
  ggplot(aes(x = window, y = r2_combined)) +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_point(size = 2, color = "steelblue") +
  geom_vline(xintercept = c(5, 8, 10), linetype = "dashed", alpha = 0.3) +
  labs(
    title = "Predictive Power by Rolling Window Size",
    subtitle = "R-squared: rolling offensive avg + opponent defensive avg → next game total FP",
    x = "Number of prior games in rolling window",
    y = "R-squared"
  ) +
  scale_x_continuous(breaks = 1:20) +
  theme_minimal()
```

### Step 14: Model comparison — what actually predicts fantasy production?

Before building a model, we need to understand what information is valuable. We compare rolling averages (our stats-based approach) against Vegas implied totals (the market's prediction). Training on 2016-2024, testing on 2025.

```{r model-setup}
# Add features needed for modeling
game_log <- game_log |>
  mutate(
    is_home = as.integer(location == "home"),
    team_spread = if_else(location == "home", spread_line, -spread_line)
  )

# Filter to complete cases for fair model comparison
model_data <- game_log |>
  filter(!is.na(roll_8_off_total), !is.na(opp_roll_8_def_total), !is.na(implied_total))

train <- model_data |> filter(season <= 2024)
test <- model_data |> filter(season == 2025)

cat("Training:", nrow(train), "games (2016-2024)\n")
cat("Testing:", nrow(test), "games (2025)\n")
```

```{r model-comparison}
# Head-to-head: rolling averages vs Vegas vs combined
models <- list(
  "Rolling avg only"   = lm(team_fp_total ~ roll_8_off_total + opp_roll_8_def_total, data = train),
  "Vegas only"         = lm(team_fp_total ~ implied_total, data = train),
  "Vegas + home/away"  = lm(team_fp_total ~ implied_total + is_home, data = train),
  "Rolling + Vegas"    = lm(team_fp_total ~ roll_8_off_total + opp_roll_8_def_total + implied_total, data = train),
  "Full"               = lm(team_fp_total ~ roll_8_off_total + opp_roll_8_def_total + implied_total + is_home, data = train)
)

model_comparison <- map_dfr(names(models), function(name) {
  m <- models[[name]]
  preds <- predict(m, newdata = test)
  tibble(
    model = name,
    train_r2 = round(summary(m)$r.squared, 4),
    test_r2 = round(cor(preds, test$team_fp_total)^2, 4),
    test_mae = round(mean(abs(preds - test$team_fp_total)), 1)
  )
})

cat("=== Total FP Model Comparison ===\n\n")
print(model_comparison)

# Key finding: Vegas dominates. Show the full model coefficients to prove it.
cat("\nFull model coefficients (are rolling avgs significant after adding Vegas?):\n")
full_coefs <- summary(models[["Full"]])$coefficients
for (i in 1:nrow(full_coefs)) {
  sig <- ifelse(full_coefs[i, 4] < 0.001, "***",
         ifelse(full_coefs[i, 4] < 0.05, "*", ""))
  cat(sprintf("  %-25s  coef=%6.3f  p=%.3f %s\n",
    rownames(full_coefs)[i], full_coefs[i, 1], full_coefs[i, 4], sig))
}

cat("\nKey insight: implied_total alone explains ~19% of variance.\n")
cat("Rolling averages add nothing — Vegas already bakes in team quality and matchup.\n")
```

```{r vegas-calibration}
# Calibration plot: Vegas implied total vs actual FP
test$pred_vegas <- predict(models[["Vegas only"]], newdata = test)
test_r2 <- round(cor(test$pred_vegas, test$team_fp_total)^2, 3)
test_mae <- round(mean(abs(test$pred_vegas - test$team_fp_total)), 1)

test |>
  ggplot(aes(x = pred_vegas, y = team_fp_total)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  geom_smooth(method = "lm", se = FALSE, color = "steelblue") +
  labs(
    title = "Vegas Implied Total vs Actual Team FP (2025)",
    subtitle = paste("R² =", test_r2, "| MAE =", test_mae, "FP"),
    x = "Predicted FP (from implied total)",
    y = "Actual FP"
  ) +
  theme_minimal()
```

### Step 15: Category-level prediction — where rolling averages add value

Vegas lines predict total production well but don't tell us how production splits between passing, rushing, and receiving. This is where rolling averages become useful — not for "how much" but for "what kind" of production.

```{r category-model-comparison}
# For each category: compare Vegas-only vs Rolling-only vs Combined
cat("=== Category Models: Vegas vs Rolling vs Combined (test 2025) ===\n\n")

cat_comparison <- map_dfr(c("passing", "rushing", "receiving"), function(cat_name) {
  actual <- paste0("team_fp_", cat_name)
  off_col <- paste0("roll_8_off_", cat_name)
  opp_col <- paste0("opp_roll_8_def_", cat_name)

  m_vegas <- lm(as.formula(paste(actual, "~ implied_total")), data = train)
  m_roll  <- lm(as.formula(paste(actual, "~", off_col, "+", opp_col)), data = train)
  m_combo <- lm(as.formula(paste(actual, "~", off_col, "+", opp_col, "+ implied_total")), data = train)

  pv <- predict(m_vegas, newdata = test)
  pr <- predict(m_roll, newdata = test)
  pc <- predict(m_combo, newdata = test)

  # P-values for rolling avg features in combined model
  combo_coefs <- summary(m_combo)$coefficients

  tibble(
    category = cat_name,
    vegas_r2 = round(cor(pv, test[[actual]])^2, 4),
    rolling_r2 = round(cor(pr, test[[actual]])^2, 4),
    combined_r2 = round(cor(pc, test[[actual]])^2, 4),
    own_roll_p = round(combo_coefs[off_col, 4], 3),
    opp_def_p = round(combo_coefs[opp_col, 4], 3)
  )
})

print(cat_comparison)

cat("\nFindings:\n")
cat("  - Vegas dominates for passing and receiving (game total drives these)\n")
cat("  - Rolling avgs add most for RUSHING — team tendencies matter independently\n")
cat("  - Team's own rolling avg is always significant (p<0.001)\n")
cat("  - Opponent defensive rolling avg is significant only for rushing\n")
```

### Step 16: Two-step model — Vegas total x rolling category shares

The best approach separates two questions: (1) how many total FP will this team score? (answer: use Vegas) and (2) how will that production split across categories? (answer: use rolling averages). Multiply the two for category-level predictions.

```{r two-step-model}
# Compute rolling category shares
model_data <- model_data |>
  mutate(
    roll_8_total = roll_8_off_passing + roll_8_off_rushing + roll_8_off_receiving,
    roll_pass_share = roll_8_off_passing / roll_8_total,
    roll_rush_share = roll_8_off_rushing / roll_8_total,
    roll_recv_share = roll_8_off_receiving / roll_8_total
  )

train <- model_data |> filter(season <= 2024)
test <- model_data |> filter(season == 2025)

# Step 1: predict total FP from Vegas
m_total <- lm(team_fp_total ~ implied_total, data = train)
test$pred_total <- predict(m_total, newdata = test)

# Step 2: predict category shares from rolling avgs + opponent defense
# Step 3: multiply for category predictions
cat("=== Two-Step Model vs Direct Model (test 2025) ===\n\n")

two_step_results <- map_dfr(c("passing", "rushing", "receiving"), function(cat_name) {
  actual <- paste0("team_fp_", cat_name)
  off_col <- paste0("roll_8_off_", cat_name)
  opp_col <- paste0("opp_roll_8_def_", cat_name)

  share_name <- switch(cat_name,
    passing = "pass_share", rushing = "rush_share", receiving = "recv_share")
  roll_share <- switch(cat_name,
    passing = "roll_pass_share", rushing = "roll_rush_share", receiving = "roll_recv_share")

  # Actual shares for training
  train$actual_share <- train[[actual]] / train$team_fp_total
  test$actual_share <- test[[actual]] / test$team_fp_total

  # Two-step: predict share, multiply by predicted total
  m_share <- lm(as.formula(paste("actual_share ~", roll_share, "+", opp_col)), data = train)
  test$pred_share <- predict(m_share, newdata = test)
  pred_twostep <- test$pred_total * test$pred_share

  # Direct: single model with rolling avgs + Vegas
  m_direct <- lm(as.formula(paste(actual, "~", off_col, "+", opp_col, "+ implied_total")), data = train)
  pred_direct <- predict(m_direct, newdata = test)

  tibble(
    category = cat_name,
    direct_r2 = round(cor(pred_direct, test[[actual]])^2, 4),
    direct_mae = round(mean(abs(pred_direct - test[[actual]])), 1),
    twostep_r2 = round(cor(pred_twostep, test[[actual]])^2, 4),
    twostep_mae = round(mean(abs(pred_twostep - test[[actual]])), 1)
  )
})

print(two_step_results)

cat("\nThe two-step approach (Vegas total x rolling shares) slightly outperforms\n")
cat("the direct model across all categories. This decomposition is also more\n")
cat("interpretable: 'how much production' is separate from 'what kind'.\n")
```

```{r prediction-calibration}
# Final calibration: two-step predictions vs actuals for each category
# Using the best approach (two-step)
test$pred_pass <- test$pred_total * predict(
  lm(I(team_fp_passing / team_fp_total) ~ roll_pass_share + opp_roll_8_def_passing, data = train),
  newdata = test)
test$pred_rush <- test$pred_total * predict(
  lm(I(team_fp_rushing / team_fp_total) ~ roll_rush_share + opp_roll_8_def_rushing, data = train),
  newdata = test)
test$pred_recv <- test$pred_total * predict(
  lm(I(team_fp_receiving / team_fp_total) ~ roll_recv_share + opp_roll_8_def_receiving, data = train),
  newdata = test)

# Residual analysis by game environment
cat("=== Residual Analysis by Implied Total Bucket (2025) ===\n\n")
test |>
  filter(!is.na(implied_total)) |>
  mutate(
    resid_total = team_fp_total - pred_total,
    impl_bucket = cut(implied_total,
      breaks = c(0, 20, 23, 26, 35),
      labels = c("< 20", "20-23", "23-26", "26+"))
  ) |>
  group_by(impl_bucket) |>
  summarise(
    n = n(),
    mean_pred = round(mean(pred_total), 1),
    mean_actual = round(mean(team_fp_total), 1),
    mean_resid = round(mean(resid_total), 1),
    .groups = "drop"
  ) |>
  print()

# Validation: residuals should be roughly centered at 0 in each bucket
cat("\nResiduals near 0 across buckets = well-calibrated model.\n")
```

```{r summary-findings}
cat("===========================================================\n")
cat("  SUMMARY: Predicting Team Fantasy Production by Category   \n")
cat("===========================================================\n\n")

cat("1. TOTAL FP: Vegas implied total is the single best predictor (R² ≈ 0.19).\n")
cat("   Rolling averages add nothing once Vegas is included — the market\n")
cat("   already incorporates team quality, matchup, and context.\n\n")

cat("2. CATEGORY SPLITS: Rolling averages are useful here. Team tendencies\n")
cat("   (pass-heavy vs run-heavy) persist and Vegas doesn't capture them.\n")
cat("   - Receiving share is most predictable (teams have consistent pass/run ratios)\n")
cat("   - Rushing is where opponent defense matters most (p<0.001)\n\n")

cat("3. BEST APPROACH: Two-step model\n")
cat("   Step 1: Predict total FP from Vegas implied total\n")
cat("   Step 2: Predict category shares from 8-game rolling averages\n")
cat("   Step 3: Multiply total × share = category prediction\n\n")

cat("4. PRACTICAL LIMITS: Even the best model explains ~13% of category variance.\n")
cat("   Individual game outcomes are inherently noisy. The value is in identifying\n")
cat("   systematic edges — the other 87% is what makes DFS a game of skill + luck.\n\n")

cat("5. ROLLING WINDOW: 5-10 games is the sweet spot. Fewer is too noisy,\n")
cat("   more than ~16 starts capturing stale information.\n")
```

## Variance Analysis for Simulation

For a 2026 season simulation, we need more than just mean projections — we need to know how much each team's weekly output fluctuates. Key questions:

1. How much of the variance in weekly FP is **between teams** vs **within teams** (game-to-game noise)?
2. Is a team's variance level a **persistent trait** or just randomness?
3. Are passing, rushing, and receiving deviations **correlated** within a game?
4. What variance parameters should we plug into a simulation?

### Step 17: Variance decomposition

We decompose total variance into between-team and within-team components using ANOVA / ICC. The intra-class correlation (ICC) tells us what fraction of weekly variance is "team identity" — the rest is game-to-game noise that a simulation must build in.

```{r variance-decomposition}
# Focus on games with complete data
var_data <- game_log |>
  filter(!is.na(opponent), season >= 2020)

# ICC via one-way random-effects ANOVA
# ICC = (MSb - MSw) / (MSb + (k-1)*MSw) where k = avg group size
compute_icc <- function(data, value_col, group_col = "recent_team") {
  aov_fit <- aov(as.formula(paste(value_col, "~", group_col)), data = data)
  ss <- summary(aov_fit)[[1]]
  msb <- ss["Mean Sq"][[1]][1]  # between-group mean square
  msw <- ss["Mean Sq"][[1]][2]  # within-group mean square
  k <- nrow(data) / length(unique(data[[group_col]]))
  icc <- (msb - msw) / (msb + (k - 1) * msw)
  list(icc = icc, msb = msb, msw = msw, k = round(k, 1))
}

# Compute ICC for each category (offense and defense)
cats <- c("team_fp_passing", "team_fp_rushing", "team_fp_receiving", "team_fp_total",
          "def_allowed_fp_passing", "def_allowed_fp_rushing",
          "def_allowed_fp_receiving", "def_allowed_fp_total")

icc_results <- map_dfr(cats, function(col) {
  res <- compute_icc(var_data |> filter(!is.na(.data[[col]])), col)
  tibble(
    metric = col,
    icc = round(res$icc, 3),
    between_ms = round(res$msb, 1),
    within_ms = round(res$msw, 1),
    weekly_sd = round(sqrt(res$msw), 1)
  )
})

cat("=== Intra-Class Correlation (ICC) by Category (2020-2025) ===\n")
cat("ICC = fraction of weekly variance explained by 'which team'\n")
cat("Higher ICC = more team-driven, less random noise\n\n")
print(icc_results)

cat("\nInterpretation:\n")
cat("- ICC around 0.10-0.15 means ~85-90% of weekly variance is NOISE, not team quality\n")
cat("- This is expected — football is inherently high-variance game to game\n")
cat("- The simulation must primarily model game-level noise, not team differences\n")
```

```{r variance-by-season}
# Does this hold across individual seasons?
icc_by_season <- map_dfr(2020:2025, function(yr) {
  d <- var_data |> filter(season == yr)
  map_dfr(c("team_fp_total", "team_fp_passing", "team_fp_rushing", "team_fp_receiving"), function(col) {
    res <- compute_icc(d |> filter(!is.na(.data[[col]])), col)
    tibble(season = yr, metric = col, icc = round(res$icc, 3))
  })
})

cat("=== ICC by Season (is the team signal stable across years?) ===\n")
icc_by_season |>
  pivot_wider(names_from = metric, values_from = icc) |>
  print()
```

### Step 18: Is team-level variance a persistent trait?

Some teams feel "boom or bust" while others are steady. But is this real or just sampling? We test two ways: (1) YoY correlation of team SDs, and (2) Levene's test for equality of variances.

```{r variance-persistence}
# Compute team-season SDs
team_sds <- game_log |>
  filter(!is.na(opponent)) |>
  group_by(recent_team, season) |>
  summarise(
    n_games = n(),
    sd_total = sd(team_fp_total),
    sd_passing = sd(team_fp_passing),
    sd_rushing = sd(team_fp_rushing),
    sd_receiving = sd(team_fp_receiving),
    sd_def_total = sd(def_allowed_fp_total, na.rm = TRUE),
    sd_def_passing = sd(def_allowed_fp_passing, na.rm = TRUE),
    sd_def_rushing = sd(def_allowed_fp_rushing, na.rm = TRUE),
    sd_def_receiving = sd(def_allowed_fp_receiving, na.rm = TRUE),
    .groups = "drop"
  )

# YoY correlation: does a team's SD in year N predict SD in year N+1?
sd_yoy <- team_sds |>
  inner_join(
    team_sds |> mutate(season = season - 1) |>
      rename_with(~ paste0("next_", .x), -c(recent_team, season)),
    by = c("recent_team", "season")
  )

cat("=== YoY Correlation of Team Standard Deviations ===\n")
cat("(Does a 'boom/bust' team stay boom/bust the next year?)\n\n")
sd_cols <- c("sd_total", "sd_passing", "sd_rushing", "sd_receiving",
             "sd_def_total", "sd_def_passing", "sd_def_rushing", "sd_def_receiving")
for (col in sd_cols) {
  next_col <- paste0("next_", col)
  r <- cor(sd_yoy[[col]], sd_yoy[[next_col]], use = "complete.obs")
  cat(sprintf("  %-18s r = %+.3f\n", col, r))
}

cat("\nInterpretation:\n")
cat("  r near 0 → variance is NOT persistent (mostly noise)\n")
cat("  r > 0.20 → some signal — team style/roster affects variance\n")
```

```{r levene-test}
# Levene's test: are team variances significantly different?
# Uses median-based (Brown-Forsythe) variant which is robust to non-normality
levene_test <- function(data, value_col) {
  vals <- data[[value_col]]
  groups <- data[["recent_team"]]
  group_medians <- tapply(vals, groups, median, na.rm = TRUE)
  abs_devs <- abs(vals - group_medians[groups])
  aov_result <- summary(aov(abs_devs ~ groups))
  f_val <- aov_result[[1]]["F value"][[1]][1]
  p_val <- aov_result[[1]]["Pr(>F)"][[1]][1]
  list(f = f_val, p = p_val)
}

cat("=== Brown-Forsythe Test: Do Teams Have Different Variances? ===\n")
cat("(Null: all teams have equal variance)\n\n")

# Test by season to avoid confounding
bf_results <- map_dfr(c("team_fp_total", "team_fp_passing", "team_fp_rushing",
                         "team_fp_receiving"), function(col) {
  # Pool across 2020-2025
  d <- game_log |> filter(season >= 2020, !is.na(.data[[col]]))
  res <- levene_test(d, col)
  tibble(
    metric = col,
    f_stat = round(res$f, 2),
    p_value = round(res$p, 4),
    significant = ifelse(res$p < 0.05, "YES", "no")
  )
})
print(bf_results)

cat("\nIf p < 0.05, teams DO have statistically different variances.\n")
cat("But statistical significance ≠ practical significance. Check effect sizes below.\n")
```

```{r variance-spread-visual}
# Visualize: team SD distributions for 2024
team_sds |>
  filter(season == 2024) |>
  select(recent_team, sd_total, sd_passing, sd_rushing, sd_receiving) |>
  pivot_longer(-recent_team, names_to = "category", values_to = "sd") |>
  mutate(category = str_replace(category, "sd_", ""),
         category = str_to_title(category)) |>
  ggplot(aes(x = reorder(recent_team, -sd), y = sd, fill = category)) +
  geom_col(position = "dodge") +
  labs(
    title = "Team Weekly Standard Deviation by Category (2024)",
    subtitle = "Higher = more boom/bust week to week",
    x = NULL, y = "Weekly SD (fantasy points)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7))
```

### Step 19: Category correlation structure within games

When a team has a "boom" game in passing, does rushing also boom? Or do they substitute? This correlation structure matters for simulation — if categories are negatively correlated, total variance is lower than you'd get simulating them independently.

```{r category-correlations}
# Within-game deviations from team-season means
game_devs <- game_log |>
  filter(!is.na(opponent), season >= 2020) |>
  group_by(recent_team, season) |>
  mutate(
    dev_passing = team_fp_passing - mean(team_fp_passing),
    dev_rushing = team_fp_rushing - mean(team_fp_rushing),
    dev_receiving = team_fp_receiving - mean(team_fp_receiving),
    dev_def_passing = def_allowed_fp_passing - mean(def_allowed_fp_passing, na.rm = TRUE),
    dev_def_rushing = def_allowed_fp_rushing - mean(def_allowed_fp_rushing, na.rm = TRUE),
    dev_def_receiving = def_allowed_fp_receiving - mean(def_allowed_fp_receiving, na.rm = TRUE)
  ) |>
  ungroup()

# Offense: passing vs rushing vs receiving deviations
cat("=== Within-Game Category Correlation (Offense) ===\n")
cat("(Deviations from team-season mean)\n\n")
off_devs <- game_devs |>
  select(dev_passing, dev_rushing, dev_receiving) |>
  filter(complete.cases(pick(everything())))
off_cor <- cor(off_devs)
cat("  Passing vs Rushing:   ", round(off_cor["dev_passing", "dev_rushing"], 3), "\n")
cat("  Passing vs Receiving: ", round(off_cor["dev_passing", "dev_receiving"], 3), "\n")
cat("  Rushing vs Receiving: ", round(off_cor["dev_rushing", "dev_receiving"], 3), "\n")

cat("\nDefense:\n")
def_devs <- game_devs |>
  select(dev_def_passing, dev_def_rushing, dev_def_receiving) |>
  filter(complete.cases(pick(everything())))
def_cor <- cor(def_devs)
cat("  Pass vs Rush allowed:  ", round(def_cor["dev_def_passing", "dev_def_rushing"], 3), "\n")
cat("  Pass vs Recv allowed:  ", round(def_cor["dev_def_passing", "dev_def_receiving"], 3), "\n")
cat("  Rush vs Recv allowed:  ", round(def_cor["dev_def_rushing", "dev_def_receiving"], 3), "\n")
```

```{r correlation-implication}
# What does this mean for simulation?
# If we simulate categories independently with SDs from above,
# the total SD would be sqrt(sd_pass^2 + sd_rush^2 + sd_recv^2)
# But with correlations, total SD = sqrt(var_p + var_r + var_v + 2*cov_pr + 2*cov_pv + 2*cov_rv)

# Calculate these for typical team
avg_sds <- game_devs |>
  summarise(
    sd_p = sd(dev_passing, na.rm = TRUE),
    sd_r = sd(dev_rushing, na.rm = TRUE),
    sd_v = sd(dev_receiving, na.rm = TRUE)
  )

r_pr <- off_cor["dev_passing", "dev_rushing"]
r_pv <- off_cor["dev_passing", "dev_receiving"]
r_rv <- off_cor["dev_rushing", "dev_receiving"]

sd_independent <- sqrt(avg_sds$sd_p^2 + avg_sds$sd_r^2 + avg_sds$sd_v^2)
sd_correlated <- sqrt(
  avg_sds$sd_p^2 + avg_sds$sd_r^2 + avg_sds$sd_v^2 +
  2 * r_pr * avg_sds$sd_p * avg_sds$sd_r +
  2 * r_pv * avg_sds$sd_p * avg_sds$sd_v +
  2 * r_rv * avg_sds$sd_r * avg_sds$sd_v
)

cat("\n=== Simulation Impact ===\n")
cat("Average weekly SDs:  passing =", round(avg_sds$sd_p, 1),
    " rushing =", round(avg_sds$sd_r, 1),
    " receiving =", round(avg_sds$sd_v, 1), "\n")
cat("If categories were independent: total SD =", round(sd_independent, 1), "\n")
cat("With actual correlations:       total SD =", round(sd_correlated, 1), "\n")
cat("Actual observed total SD:       ", round(sd(game_devs$team_fp_total - mean(game_devs$team_fp_total), na.rm = TRUE), 1), "\n")

cat("\nKey: passing & receiving are strongly correlated (both driven by pass volume).\n")
cat("Rushing is weakly/negatively correlated with passing (substitution effect).\n")
cat("This means simulating categories independently would OVERESTIMATE total variance\n")
cat("if pass-recv correlation is positive, or get it roughly right if they offset.\n")
```

### Step 20: Simulation variance parameters

Now we derive the actual numbers a simulation needs. For each team, we decompose their weekly output into: (1) a baseline expectation (from our model), (2) a game-environment adjustment (from Vegas lines), and (3) a residual (the noise to simulate). We characterize the residual distribution and check whether it's normal or fat-tailed.

```{r residual-distribution}
# Use our best model (Vegas implied total) to get residuals
# These residuals represent the "noise" a simulation must generate
sim_data <- game_log |>
  filter(!is.na(implied_total), season >= 2020)

m_total <- lm(team_fp_total ~ implied_total, data = sim_data)
sim_data$resid_total <- residuals(m_total)

# Category models
m_pass <- lm(team_fp_passing ~ implied_total + roll_8_off_passing + opp_roll_8_def_passing,
             data = sim_data |> filter(!is.na(roll_8_off_passing)))
m_rush <- lm(team_fp_rushing ~ implied_total + roll_8_off_rushing + opp_roll_8_def_rushing,
             data = sim_data |> filter(!is.na(roll_8_off_rushing)))
m_recv <- lm(team_fp_receiving ~ implied_total + roll_8_off_receiving + opp_roll_8_def_receiving,
             data = sim_data |> filter(!is.na(roll_8_off_receiving)))

cat("=== Residual (Noise) Distribution for Simulation ===\n\n")
cat("Total FP residuals:\n")
cat("  Mean:", round(mean(sim_data$resid_total), 2), "(should be ~0)\n")
cat("  SD:  ", round(sd(sim_data$resid_total), 1), "\n")
cat("  Skew:", round(moments::skewness(sim_data$resid_total), 3), "\n")
cat("  Kurt:", round(moments::kurtosis(sim_data$resid_total), 3), "(normal = 3)\n\n")

# Check for fat tails via quantile comparison
observed_q <- quantile(sim_data$resid_total, c(0.01, 0.05, 0.10, 0.90, 0.95, 0.99))
normal_q <- qnorm(c(0.01, 0.05, 0.10, 0.90, 0.95, 0.99), 0, sd(sim_data$resid_total))

cat("Tail comparison (observed vs normal):\n")
cat(sprintf("  1st pctile:  %.1f vs %.1f (normal)\n", observed_q[1], normal_q[1]))
cat(sprintf("  5th pctile:  %.1f vs %.1f (normal)\n", observed_q[2], normal_q[2]))
cat(sprintf("  95th pctile: %.1f vs %.1f (normal)\n", observed_q[5], normal_q[5]))
cat(sprintf("  99th pctile: %.1f vs %.1f (normal)\n", observed_q[6], normal_q[6]))
```

```{r residual-qq-plot}
# QQ plot to visually assess normality
tibble(resid = sim_data$resid_total) |>
  ggplot(aes(sample = resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(
    title = "QQ Plot: Total FP Residuals (after removing Vegas signal)",
    subtitle = "Deviation from line = non-normal tails",
    x = "Theoretical (Normal)", y = "Observed"
  ) +
  theme_minimal()
```

```{r sim-params-weekly}
# Derive simulation parameters per team
# We want: for each team, what's their typical residual SD?
# (i.e., how much do they deviate from model expectations week to week?)

team_resid_sds <- sim_data |>
  group_by(recent_team) |>
  summarise(
    n_games = n(),
    resid_sd_total = sd(resid_total),
    .groups = "drop"
  )

league_resid_sd <- sd(sim_data$resid_total)

cat("=== Team-Level Residual SDs (weekly noise after model) ===\n")
cat("League average residual SD:", round(league_resid_sd, 1), "FP\n\n")
team_resid_sds |>
  mutate(vs_league = round(resid_sd_total / league_resid_sd, 2)) |>
  arrange(desc(resid_sd_total)) |>
  mutate(resid_sd_total = round(resid_sd_total, 1)) |>
  print(n = 32)

cat("\nTeams with ratio > 1.10 are more volatile than expected by model.\n")
cat("Teams with ratio < 0.90 are more predictable.\n")
cat("But recall from Step 18: team-level SD is mostly noise, not a persistent trait.\n")
```

```{r sim-params-seasonal}
# For season-long simulation: what's the expected variance of season totals?
# If weekly residuals are independent: season_SD = weekly_SD * sqrt(17)
# If there's autocorrelation: we need to adjust

# Test autocorrelation of residuals within a team-season
autocor_results <- sim_data |>
  group_by(recent_team, season) |>
  arrange(week) |>
  summarise(
    n = n(),
    lag1_cor = cor(resid_total[-n()], resid_total[-1], use = "complete.obs"),
    .groups = "drop"
  ) |>
  filter(n >= 10)  # need enough games

mean_autocor <- mean(autocor_results$lag1_cor, na.rm = TRUE)
cat("=== Autocorrelation of Weekly Residuals ===\n")
cat("Mean lag-1 autocorrelation:", round(mean_autocor, 3), "\n")
cat("Distribution:\n")
print(summary(autocor_results$lag1_cor))

cat("\nIf autocorrelation ≈ 0: weeks are roughly independent.\n")
cat("Weekly independence means season-total SD ≈ weekly SD × sqrt(17).\n\n")

# Derive season-long parameters
weekly_sd <- league_resid_sd
season_sd_independent <- weekly_sd * sqrt(17)
# With autocorrelation adjustment: SD * sqrt(n * (1 + 2*r*(n-1)/n) / (1-r^2))
# Simplified for small r: SD * sqrt(n) * sqrt(1 + 2*r)
season_sd_adjusted <- weekly_sd * sqrt(17) * sqrt(1 + 2 * mean_autocor)

cat("=== Season-Long Simulation Parameters ===\n\n")
cat("Weekly residual SD (after model):", round(weekly_sd, 1), "FP\n")
cat("Season total SD (assuming independence):", round(season_sd_independent, 1), "FP\n")
cat("Season total SD (with autocorrelation):", round(season_sd_adjusted, 1), "FP\n\n")

cat("For a 2026 simulation:\n")
cat("  1. Start with team baseline (from Step 10 projections)\n")
cat("  2. Each week, adjust for game environment (Vegas implied total)\n")
cat("  3. Add random noise ~ N(0, ", round(weekly_sd, 0), ") for total FP\n", sep = "")
cat("  4. Split into categories using rolling shares + correlated noise\n")
cat("  5. Season totals will have SD ≈", round(season_sd_independent, 0), "FP around projection\n")
```

```{r sim-params-category-detail}
# Category-level simulation parameters with correlation matrix
# These are the actual numbers you'd plug into a multivariate normal simulation

cat_sim_data <- sim_data |>
  filter(!is.na(roll_8_off_passing), !is.na(opp_roll_8_def_passing),
         !is.na(roll_8_off_rushing), !is.na(opp_roll_8_def_rushing),
         !is.na(roll_8_off_receiving), !is.na(opp_roll_8_def_receiving))

# Refit category models on this consistent dataset
m_pass_sim <- lm(team_fp_passing ~ implied_total + roll_8_off_passing + opp_roll_8_def_passing, data = cat_sim_data)
m_rush_sim <- lm(team_fp_rushing ~ implied_total + roll_8_off_rushing + opp_roll_8_def_rushing, data = cat_sim_data)
m_recv_sim <- lm(team_fp_receiving ~ implied_total + roll_8_off_receiving + opp_roll_8_def_receiving, data = cat_sim_data)

# Get category residuals
cat_sim_data <- cat_sim_data |>
  mutate(
    resid_pass = residuals(m_pass_sim),
    resid_rush = residuals(m_rush_sim),
    resid_recv = residuals(m_recv_sim)
  )

# Category SDs
cat("\n=== Category-Level Simulation Parameters ===\n\n")
cat("Weekly residual SDs (noise after model):\n")
cat("  Passing:  ", round(sd(cat_sim_data$resid_pass), 1), "FP\n")
cat("  Rushing:  ", round(sd(cat_sim_data$resid_rush), 1), "FP\n")
cat("  Receiving:", round(sd(cat_sim_data$resid_recv), 1), "FP\n")

# Correlation matrix of category residuals
cat("\nCategory residual correlation matrix:\n")
cat("(Use this for multivariate normal simulation)\n\n")
resid_cor <- cor(cat_sim_data |> select(resid_pass, resid_rush, resid_recv))
colnames(resid_cor) <- rownames(resid_cor) <- c("Passing", "Rushing", "Receiving")
print(round(resid_cor, 3))

cat("\nSimulation recipe:\n")
cat("  1. Draw (pass, rush, recv) residuals from MVN with this correlation matrix\n")
cat("  2. Scale by the SDs above\n")
cat("  3. Add to category predictions from the two-step model\n")
cat("  4. This preserves the pass/recv co-movement and rush substitution effect\n")
```

```{r variance-summary}
cat("==================================================================\n")
cat("  SUMMARY: Variance Parameters for 2026 Simulation                \n")
cat("==================================================================\n\n")

cat("WEEKLY LEVEL:\n")
cat("  - ~85-90% of week-to-week variance is noise (not team quality)\n")
cat("  - Total FP residual SD ≈", round(league_resid_sd, 0), "FP after Vegas model\n")
cat("  - Team-level variance is NOT a persistent trait (low YoY r)\n")
cat("  - Use a single league-wide noise SD, not team-specific ones\n\n")

cat("CATEGORY STRUCTURE:\n")
cat("  - Passing & receiving deviate together (r ≈", round(resid_cor["Passing","Receiving"], 2), ")\n")
cat("  - Rushing substitutes for passing when games go off-script\n")
cat("  - Simulate categories jointly using multivariate normal\n\n")

cat("SEASON LEVEL:\n")
cat("  - Weekly residuals are approximately independent (autocor ≈",
    round(mean_autocor, 2), ")\n")
cat("  - Season-total SD ≈", round(season_sd_independent, 0),
    "FP (weekly SD × √17)\n")
cat("  - A team projected for 70 FP/game → 1,190 season total\n")
cat("    with 68% CI: [", round(1190 - season_sd_independent, 0), ",",
    round(1190 + season_sd_independent, 0), "]\n\n")

cat("SIMULATION STEPS:\n")
cat("  1. Set team baselines from Step 10 regression projections\n")
cat("  2. For each simulated week:\n")
cat("     a. Set expected total from Vegas implied total (or baseline if no line)\n")
cat("     b. Set expected category shares from 8-game rolling averages\n")
cat("     c. Draw correlated (pass, rush, recv) noise from MVN\n")
cat("     d. Add noise to get simulated weekly category FP\n")
cat("  3. Sum 17 weeks for season totals\n")
cat("  4. Repeat 10,000+ times for confidence intervals\n")
```

## Draft Analysis Ideas

```{r draft-analysis}
# Example: pick duration analysis (slow vs fast drafters)
# Example: positional run detection
# Example: value over ADP by round

# Your code here...
```

---

```{r cleanup, include=FALSE}
dbDisconnect(con)
```

